{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "Data Preparation Tutorial with Diabetes Dataset",
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "import kagglehub\n",
        "furkannakdagg_diabetes_path = kagglehub.dataset_download('furkannakdagg/diabetes')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "5aRCf5WGQzkh"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FEATURE ENGINEERING & DATA PRE-PROCESSING\n",
        "\n",
        "**Aim of the Notebook**: Feature engineering and Data Pre-Processing are the most significant topics in data science. In a general opinion, %80 of the work is data preprocessing, and %20 of the work is modeling in a machine learning project, therefore, there are issues to be considered. The aim of this notebook is to be a tutorial on the approach to data preparation.\n",
        "\n",
        "First of all, check the what is the difference between feature engineering and data preprocessing.\n",
        "\n",
        "**Feature Engineering:** The work performed on the features (such as preprocessing, generating a new variable, etc.), and generating variables from raw data.\n",
        "\n",
        "**Data Preprocessing:** The process of making the data suitable before implementing a model.\n",
        "\n",
        "Actually, the aim is similar, make the dataset become much more suitable, and prepare it for the model. The difference can be shown in a figure:\n",
        "\n",
        "![Diagram.png](attachment:300022a5-a86b-4c85-aee0-34b1902a5ee9.png)\n",
        "\n",
        "In this step, there are 4 topics to consider and handle to prepare the dataset.\n",
        "1. Outliers\n",
        "2. Missing Values\n",
        "3. Feature Extraction\n",
        "4. Encoding & Scaling\n",
        "\n",
        "All of these are the steps of data preprocessing and feature engineering, and each of them will be explained in the notebook.\n",
        "\n",
        "Obviously, before the process, the dataset has to be explored, and the variables have to be understood. That means the dataset has to be prepared before preparing. Make ready to your popcorns for the movie named \"Inception on Data Science\"! Of course, it does not that hard, but there are rules and steps to be followed in order to not get lost.\n",
        "\n",
        "## Table of Contents\n",
        "\n",
        "### 1. [First look to the dataset](#1)\n",
        "### 2. [EDA (Exploratory Data Analysis)](#2)\n",
        "* 2.1 [Analyse outliers and missing values](#2.1)\n",
        "\n",
        "### 3. [Pre-processing (Outliers and Missing Values)](#3)\n",
        "### 4. [Feature Extraction](#4)\n",
        "### 5. [Encoding & Scaling](#5)\n",
        "### 6. [Modeling](#6)"
      ],
      "metadata": {
        "id": "bBsSMJoPQzkh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"1\"></a> <br>\n",
        "# 1. First look to the Dataset\n",
        "\n",
        "**Business Problem:** It is desired to develop a machine learning model that can predict whether people have diabetes when their characteristics are specified. You are expected to perform the necessary data analysis and feature engineering steps before developing the model.\n",
        "\n",
        "**Story of Dataset:** The dataset is part of the large dataset held at the National Institutes of Diabetes-Digestive-Kidney Diseases in the USA. Data used for diabetes research on Pima Indian women aged 21 and over living in Phoenix, the 5th largest city of the State of Arizona in the USA.\n",
        "\n",
        "**Variables:** The target variable is specified as **\"outcome\"**; **1** indicates **positive** diabetes test result, **0** indicates **negative**.\n",
        "\n",
        "\n",
        "- **Pregnancies:** The number of pregnancies\n",
        "- **Glucose:** 2-hour plasma glucose concentration in the oral glucose tolerance test\n",
        "- **Blood Pressure:** Blood Pressure (Small blood pressure) (mmHg)\n",
        "- **SkinThickness:** Skin Thickness\n",
        "- **Insulin:** 2-hour serum insulin (mu U/ml)\n",
        "- **DiabetesPedigreeFunction:** A function that calculates the probability of having diabetes according to the descendants\n",
        "- **BMI:** Body mass index\n",
        "- **Age:** Age (year)\n",
        "- **Outcome:** Have the disease (1) or not (0)"
      ],
      "metadata": {
        "id": "ld7TPHBtQzkj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"2\"></a> <br>\n",
        "# 2. EDA (Exploratory Data Analysis)\n",
        "\n",
        "This is the part of the explore the dataset. The outliers and missing value analyses will be done in this part but the only point is to observe these in this part. The operation and editing for these will be done in Part 3: Pre-processing."
      ],
      "metadata": {
        "id": "JyitnnupQzkj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from matplotlib import pyplot as plt\n",
        "# !pip install missingno\n",
        "import missingno as msno\n",
        "from datetime import date\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import LocalOutlierFactor\n",
        "from sklearn.preprocessing import MinMaxScaler, LabelEncoder, StandardScaler, RobustScaler\n",
        "\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2022-08-02T16:08:11.931368Z",
          "iopub.execute_input": "2022-08-02T16:08:11.931947Z",
          "iopub.status.idle": "2022-08-02T16:08:11.943067Z",
          "shell.execute_reply.started": "2022-08-02T16:08:11.931865Z",
          "shell.execute_reply": "2022-08-02T16:08:11.94167Z"
        },
        "trusted": true,
        "id": "r-3rRU8-Qzkj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adjustment of visibility of Datafreames\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
        "pd.set_option('display.width', 500)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-02T16:08:11.946304Z",
          "iopub.execute_input": "2022-08-02T16:08:11.947051Z",
          "iopub.status.idle": "2022-08-02T16:08:11.953714Z",
          "shell.execute_reply.started": "2022-08-02T16:08:11.947014Z",
          "shell.execute_reply": "2022-08-02T16:08:11.952487Z"
        },
        "trusted": true,
        "id": "NrRwbVNyQzkk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"../input/diabetes/diabetes.csv\")\n",
        "df.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-02T16:08:12.12962Z",
          "iopub.execute_input": "2022-08-02T16:08:12.130324Z",
          "iopub.status.idle": "2022-08-02T16:08:12.150177Z",
          "shell.execute_reply.started": "2022-08-02T16:08:12.130267Z",
          "shell.execute_reply": "2022-08-02T16:08:12.148761Z"
        },
        "trusted": true,
        "id": "0MQdfY4nQzkk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"##################### Shape #####################\")\n",
        "print(df.shape)\n",
        "print(\"##################### Types #####################\")\n",
        "print(df.dtypes)\n",
        "print(\"##################### NA #####################\")\n",
        "print(df.isnull().sum())\n",
        "print(\"##################### Quantiles #####################\")\n",
        "print(df.quantile([0, 0.05, 0.50, 0.95, 0.99, 1]).T)  # sayısal değişkenlerin dağılım bilgisi"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-02T16:08:12.159389Z",
          "iopub.execute_input": "2022-08-02T16:08:12.15983Z",
          "iopub.status.idle": "2022-08-02T16:08:12.178348Z",
          "shell.execute_reply.started": "2022-08-02T16:08:12.159795Z",
          "shell.execute_reply": "2022-08-02T16:08:12.177219Z"
        },
        "trusted": true,
        "id": "b14-eR39Qzkl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe().T"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-02T16:08:12.331905Z",
          "iopub.execute_input": "2022-08-02T16:08:12.333119Z",
          "iopub.status.idle": "2022-08-02T16:08:12.379869Z",
          "shell.execute_reply.started": "2022-08-02T16:08:12.333071Z",
          "shell.execute_reply": "2022-08-02T16:08:12.378318Z"
        },
        "trusted": true,
        "id": "eq6blklWQzkl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we looked at the data and checked the variables. The first step is to go deeper and understand the variables. Here, the questions have to be asked: \"which variables are _really_ numerical, and which of these are categorical? Is there any cardinal variable?\".\n",
        "\n",
        "For instance, the target variable, Outcome, seems as a numerical variable, but it is known that it is categorical since this column only includes 1 and 0(disease or no disease). These kinds of variables have to be considered categorical.\n",
        "\n",
        "Similarly, if there is a categorical variable that includes the names of the patients in a dataset, there would be considered a cardinal dataset because a name cannot carry any information. This is not valid for our dataset because there are no such columns, but it had to be considered if the dataset had.\n",
        "\n",
        "Let's write a function to differentiate the types of variables(grab_col_names). The values are open to comment. In this problem, I considered that if a numerical variable has less than 10 different values, it is actually categorical. If a categorical value has more than 20 different values, it is cardinal. Since our all variables are numerical, there will not be any cardinal variable."
      ],
      "metadata": {
        "id": "NjzZa9QDQzkm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def grab_col_names(dataframe, cat_th=10, car_th=20):\n",
        "    # cat_cols, cat_but_car\n",
        "    cat_cols = [col for col in dataframe.columns if dataframe[col].dtypes == \"O\"]\n",
        "    num_but_cat = [col for col in dataframe.columns if dataframe[col].nunique() < cat_th and\n",
        "                   dataframe[col].dtypes != \"O\"]\n",
        "    cat_but_car = [col for col in dataframe.columns if dataframe[col].nunique() > car_th and\n",
        "                   dataframe[col].dtypes == \"O\"]\n",
        "    cat_cols = cat_cols + num_but_cat\n",
        "    cat_cols = [col for col in cat_cols if col not in cat_but_car]\n",
        "\n",
        "    # num_cols\n",
        "    num_cols = [col for col in dataframe.columns if dataframe[col].dtypes != \"O\"]\n",
        "    num_cols = [col for col in num_cols if col not in num_but_cat]\n",
        "\n",
        "    # report\n",
        "    print(f\"Observations: {dataframe.shape[0]}\")\n",
        "    print(f\"Variables: {dataframe.shape[1]}\")\n",
        "    print(f'cat_cols: {len(cat_cols)}')  # the number of categorical variables\n",
        "    print(f'num_cols: {len(num_cols)}')  # the number of numerical variables\n",
        "    print(f'cat_but_car: {len(cat_but_car)}')  # the number of cardinal variables\n",
        "    print(f'num_but_cat: {len(num_but_cat)}')  # the number of categorical variables that looks numerical\n",
        "    return cat_cols, num_cols, cat_but_car\n",
        "\n",
        "\n",
        "cat_cols, num_cols, cat_but_car = grab_col_names(df)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-02T16:08:12.382818Z",
          "iopub.execute_input": "2022-08-02T16:08:12.383227Z",
          "iopub.status.idle": "2022-08-02T16:08:12.400523Z",
          "shell.execute_reply.started": "2022-08-02T16:08:12.38318Z",
          "shell.execute_reply": "2022-08-02T16:08:12.398911Z"
        },
        "trusted": true,
        "id": "xKUkUPV8Qzkm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Categorical columns:\",cat_cols)\n",
        "print(\"Numerical columns:\", num_cols)\n",
        "print(\"Cardinal columns:\", cat_but_car)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-02T16:08:12.402565Z",
          "iopub.execute_input": "2022-08-02T16:08:12.403505Z",
          "iopub.status.idle": "2022-08-02T16:08:12.411784Z",
          "shell.execute_reply.started": "2022-08-02T16:08:12.403454Z",
          "shell.execute_reply": "2022-08-02T16:08:12.410433Z"
        },
        "trusted": true,
        "id": "oHq03VTYQzkm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It can be seen that \"Outcome\" is a categorical variable, which is the target value."
      ],
      "metadata": {
        "id": "dTCu4vyBQzkn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Target variable analysis\n",
        "# The average of the numerical variables according to the target variable\n",
        "df.groupby(cat_cols)[num_cols].mean()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-02T16:08:12.548736Z",
          "iopub.execute_input": "2022-08-02T16:08:12.54913Z",
          "iopub.status.idle": "2022-08-02T16:08:12.568671Z",
          "shell.execute_reply.started": "2022-08-02T16:08:12.549097Z",
          "shell.execute_reply": "2022-08-02T16:08:12.567242Z"
        },
        "trusted": true,
        "id": "OO4eRjWuQzkn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The average of the target variable according to the categorical variables\n",
        "df[cat_cols].mean()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-02T16:08:12.572709Z",
          "iopub.execute_input": "2022-08-02T16:08:12.573555Z",
          "iopub.status.idle": "2022-08-02T16:08:12.583897Z",
          "shell.execute_reply.started": "2022-08-02T16:08:12.573519Z",
          "shell.execute_reply": "2022-08-02T16:08:12.582484Z"
        },
        "trusted": true,
        "id": "9e75v7K4Qzkn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.Outcome.value_counts())\n",
        "print(\"-------------------------------\")\n",
        "print(df[cat_cols].mean())\n",
        "print(\"-------------------------------\")\n",
        "print(\"Ratio: \")\n",
        "(df.Outcome.value_counts() / len(df)) *100"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-02T16:08:12.596626Z",
          "iopub.execute_input": "2022-08-02T16:08:12.597548Z",
          "iopub.status.idle": "2022-08-02T16:08:12.614578Z",
          "shell.execute_reply.started": "2022-08-02T16:08:12.597507Z",
          "shell.execute_reply": "2022-08-02T16:08:12.613375Z"
        },
        "trusted": true,
        "id": "GVb9TZahQzkn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(df.Outcome.value_counts() / len(df))*100"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-02T16:08:12.623748Z",
          "iopub.execute_input": "2022-08-02T16:08:12.624605Z",
          "iopub.status.idle": "2022-08-02T16:08:12.636814Z",
          "shell.execute_reply.started": "2022-08-02T16:08:12.624555Z",
          "shell.execute_reply": "2022-08-02T16:08:12.635173Z"
        },
        "trusted": true,
        "id": "J7CShrx7Qzkn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"2.1\"></a> <br>\n",
        "## 2.1 Analyse outliers and missing values\n",
        "Now, check the missing values and outliers. The only process in EDA is to analyze these values. Editing of missing values and outliers will be done in the next part.\n",
        "\n",
        "Before analyzing, firstly, an explanation of these terms will be done, the codes will be placed right after each explanation.\n",
        "\n",
        "### **Outliers**\n",
        "Values that deviate considerably from the general trend in the data are called outliers. Especially in linear problems, the effects of outliers are more severe. They have less impact on tree methods, but still, need to be considered.\n",
        "\n",
        "**How are outliers determined?:** The critical point is to determine the acceptable threshold value, which are up limit and low limit. After determining the threshold value, outliers are caught based on these values. Methods by which we can catch the threshold value:\n",
        "* Industry knowledge\n",
        "* Standard deviation approach\n",
        "* Z-score approximation\n",
        "* **<ins>Boxplot(interquartile range-IQR) method (as univariate)</ins>**\n",
        "* **<ins>LOF Method => Multivariate</ins>**\n",
        "\n",
        "We will emphasize and examine Boxplot(IQR Method) and LOF Method to analyse. After getting outliers, they may be deleted, or reassigned with thresholds(pressing outliers values to thresholds). These solution approaches will be in the next part."
      ],
      "metadata": {
        "id": "c-Wm5jtAQzkn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe().T"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-02T16:08:12.79705Z",
          "iopub.execute_input": "2022-08-02T16:08:12.797513Z",
          "iopub.status.idle": "2022-08-02T16:08:12.845876Z",
          "shell.execute_reply.started": "2022-08-02T16:08:12.797478Z",
          "shell.execute_reply": "2022-08-02T16:08:12.84433Z"
        },
        "trusted": true,
        "id": "w3Omt45mQzko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are 2 points that call attention to the first look:\n",
        "\n",
        "* \"Insulin\" has a high standard deviation, the quartile values are large, and the outlier is clear.\n",
        "\n",
        "* \"SkinThickness\" quartile distribution is uneven.\n",
        "\n",
        "For better observation, plot these."
      ],
      "metadata": {
        "id": "Met1L-6gQzko"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Boxplot\n",
        "sns.boxplot(x=df[\"Insulin\"])\n",
        "plt.show()\n",
        "\n",
        "sns.boxplot(x=df[\"SkinThickness\"])\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-02T16:08:12.84895Z",
          "iopub.execute_input": "2022-08-02T16:08:12.849536Z",
          "iopub.status.idle": "2022-08-02T16:08:13.237445Z",
          "shell.execute_reply.started": "2022-08-02T16:08:12.849486Z",
          "shell.execute_reply": "2022-08-02T16:08:13.235847Z"
        },
        "trusted": true,
        "id": "a6n8n-8mQzko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It has been mentioned that the critical point of the outliers is to determine the threshold. In the boxplot, IQR method will be used. A range named IQR(Interquartile Range) is to be determined according to quartiles, then, up limit and low limit will be found.\n",
        "In literature:\n",
        "> **IQR = Q3 - Q1**\n",
        "\n",
        "* Q3: 75% quantile\n",
        "* Q1: 25% quantile\n",
        "\n",
        "Up limit is defined as 1.5 times bigger than Q3, and the low limit is defined as a 1.5 times smaller value than Q1.\n",
        "\n",
        "![IQR.png](attachment:d4cde245-5212-48f2-a36f-788ee943c090.png)\n",
        "\n",
        "Defining Q3 and Q1 as 75th and 25th is not a rule. It is a common formula but these quartiles can be defined as 95-5, or 90-10 according to the problem. If there is a slight number of data on the dataset, separating it to 75-25% may not be appropriate because the loss of information may occur.\n",
        "\n",
        "In light of this information, let's write a function that finds the thresholds. Add the quantiles as arguments because we may want to change them.\n"
      ],
      "metadata": {
        "id": "2zetPCIAQzko"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def outlier_thresholds(dataframe, col_name, q1=0.25, q3=0.75):\n",
        "    quartile1 = dataframe[col_name].quantile(q1)\n",
        "    quartile3 = dataframe[col_name].quantile(q3)\n",
        "    interquantile_range = quartile3 - quartile1\n",
        "    up_limit = quartile3 + 1.5 * interquantile_range\n",
        "    low_limit = quartile1 - 1.5 * interquantile_range\n",
        "    return low_limit, up_limit"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-02T16:08:13.239621Z",
          "iopub.execute_input": "2022-08-02T16:08:13.240508Z",
          "iopub.status.idle": "2022-08-02T16:08:13.247529Z",
          "shell.execute_reply.started": "2022-08-02T16:08:13.240472Z",
          "shell.execute_reply": "2022-08-02T16:08:13.246017Z"
        },
        "trusted": true,
        "id": "qwdiholCQzko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in df.columns:\n",
        "    print(\"Thresholds of {} : ({:.2f}, {:.2f})\".format(i, *outlier_thresholds(df,i)))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-02T16:08:13.250326Z",
          "iopub.execute_input": "2022-08-02T16:08:13.250725Z",
          "iopub.status.idle": "2022-08-02T16:08:13.280505Z",
          "shell.execute_reply.started": "2022-08-02T16:08:13.250689Z",
          "shell.execute_reply": "2022-08-02T16:08:13.27883Z"
        },
        "trusted": true,
        "id": "mI6eopO4Qzko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Observe the outliers, it can be functioned as well, but we don't need here\n",
        "low, up = outlier_thresholds(df, \"Pregnancies\")\n",
        "df[((df[\"Pregnancies\"] < low) | (df[\"Pregnancies\"] > up))]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-02T16:08:13.282625Z",
          "iopub.execute_input": "2022-08-02T16:08:13.283002Z",
          "iopub.status.idle": "2022-08-02T16:08:13.302075Z",
          "shell.execute_reply.started": "2022-08-02T16:08:13.282968Z",
          "shell.execute_reply": "2022-08-02T16:08:13.301018Z"
        },
        "trusted": true,
        "id": "s7Y83nsmQzko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As mentioned, for Pregnancies, (Q3 + 1.5xIQR) found as 13.5, the upper of these will be considered an outlier. Also, (Q1 - 1.5xIQR) was found as -6.5. Since the minimum value of pregnancy can be 0 logically, there came nothing from the upper threshold as the outlier.\n",
        "\n",
        "**NOTE:** LOF method will give an error when there are missing values, so that, explanation and approach with LOF will be explained in part 3."
      ],
      "metadata": {
        "id": "GzgnlMDOQzko"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Missing Values**\n",
        "<ins>*“The idea of imputation is both seductive and dangerous.” -R.J.A. Little & D.B. Rubin*</ins>\n",
        "\n",
        "It refers to the situation of lack of observations. It can be solved in 3 ways:\n",
        "* Deleting\n",
        "* Value Assignment Methods (average, mode, median, etc.)\n",
        "* Predictive Methods (ML, statistical methods, etc.)\n",
        "\n",
        "One of the important issues to consider when working with missing data: The randomness of the missing data, that is, whether the missing data occur randomly or not, is the need to know. If it is random, we either delete it, fill it with the average, etc. However, there may be a dependency of the missing variable with another variable, hence, the missing may not be random. This situation is considered, as well."
      ],
      "metadata": {
        "id": "Gnu2RZkqQzko"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().values.any()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-02T16:08:13.303862Z",
          "iopub.execute_input": "2022-08-02T16:08:13.305004Z",
          "iopub.status.idle": "2022-08-02T16:08:13.313999Z",
          "shell.execute_reply.started": "2022-08-02T16:08:13.304967Z",
          "shell.execute_reply": "2022-08-02T16:08:13.312591Z"
        },
        "trusted": true,
        "id": "yn8Mf_MEQzko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-02T16:08:13.316483Z",
          "iopub.execute_input": "2022-08-02T16:08:13.316946Z",
          "iopub.status.idle": "2022-08-02T16:08:13.32957Z",
          "shell.execute_reply.started": "2022-08-02T16:08:13.316911Z",
          "shell.execute_reply": "2022-08-02T16:08:13.327948Z"
        },
        "trusted": true,
        "id": "6_z6CGRbQzko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "At the first look, it seems there is no missing value but Glucose, Insulin, etc. observation units containing a value of 0 in the variables may represent the missing value. For example, a person's glucose or insulin value can not be 0. Considering this situation, let's assign the 0 values to the relevant values as NaN and then apply the operations to the missing values.\n",
        "\n",
        "Here, the pregnancy value can be 0. It is a normal situation, hence, this column will be out of NaN."
      ],
      "metadata": {
        "id": "ZUG_APBsQzko"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# df before adding NaN\n",
        "df.head(10)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-02T16:08:13.331378Z",
          "iopub.execute_input": "2022-08-02T16:08:13.332237Z",
          "iopub.status.idle": "2022-08-02T16:08:13.353325Z",
          "shell.execute_reply.started": "2022-08-02T16:08:13.332196Z",
          "shell.execute_reply": "2022-08-02T16:08:13.351388Z"
        },
        "trusted": true,
        "id": "xvWc8BEuQzko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_cols_miss = [i for i in num_cols if i != \"Pregnancies\"]\n",
        "for i in num_cols_miss:\n",
        "    df[i] = df.apply(lambda x: np.nan if x[i] == 0 else x[i], axis=1)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-02T16:08:13.358522Z",
          "iopub.execute_input": "2022-08-02T16:08:13.359614Z",
          "iopub.status.idle": "2022-08-02T16:08:13.470154Z",
          "shell.execute_reply.started": "2022-08-02T16:08:13.359573Z",
          "shell.execute_reply": "2022-08-02T16:08:13.469046Z"
        },
        "trusted": true,
        "id": "mFa2E6MWQzko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df after adding NaN\n",
        "df.head(10)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-02T16:08:13.472275Z",
          "iopub.execute_input": "2022-08-02T16:08:13.472931Z",
          "iopub.status.idle": "2022-08-02T16:08:13.490799Z",
          "shell.execute_reply.started": "2022-08-02T16:08:13.472884Z",
          "shell.execute_reply": "2022-08-02T16:08:13.489628Z"
        },
        "trusted": true,
        "id": "nKNvewz9Qzkp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we have missing values, and analysis can be done. Check the missing values again."
      ],
      "metadata": {
        "id": "Y0Edp0UcQzkp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-02T16:08:13.492501Z",
          "iopub.execute_input": "2022-08-02T16:08:13.493241Z",
          "iopub.status.idle": "2022-08-02T16:08:13.504096Z",
          "shell.execute_reply.started": "2022-08-02T16:08:13.493177Z",
          "shell.execute_reply": "2022-08-02T16:08:13.502727Z"
        },
        "trusted": true,
        "id": "B2UrRu0YQzkp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For missing values, there a library named \"missingno\", which was imported at the beginning. This library provides visual analysis for missing values."
      ],
      "metadata": {
        "id": "ge9kUu5BQzkp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# the number of non-missing data\n",
        "msno.bar(df)\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-02T16:08:13.506693Z",
          "iopub.execute_input": "2022-08-02T16:08:13.508464Z",
          "iopub.status.idle": "2022-08-02T16:08:15.069577Z",
          "shell.execute_reply.started": "2022-08-02T16:08:13.508418Z",
          "shell.execute_reply": "2022-08-02T16:08:15.068205Z"
        },
        "trusted": true,
        "id": "fni2wwxwQzkp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "y-axis = indexes\n",
        "x-axis = variables\n",
        "black = non-missing index\n",
        "white = missing(NaN) index\n",
        "In some datasets, relativeness of missing datas on variables can be observed according to whites\n",
        "\"\"\"\n",
        "msno.matrix(df)\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-02T16:08:15.071194Z",
          "iopub.execute_input": "2022-08-02T16:08:15.071593Z",
          "iopub.status.idle": "2022-08-02T16:08:15.660562Z",
          "shell.execute_reply.started": "2022-08-02T16:08:15.07156Z",
          "shell.execute_reply": "2022-08-02T16:08:15.658994Z"
        },
        "trusted": true,
        "id": "3jMBSZw9Qzkp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "heatmap can be used rather than try to observe with eyes on matrix map\n",
        "this heatmap shows the correlation of missing values on variables\n",
        "\"\"\"\n",
        "msno.heatmap(df)\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-02T16:08:15.663231Z",
          "iopub.execute_input": "2022-08-02T16:08:15.663656Z",
          "iopub.status.idle": "2022-08-02T16:08:16.11408Z",
          "shell.execute_reply.started": "2022-08-02T16:08:15.663622Z",
          "shell.execute_reply": "2022-08-02T16:08:16.112632Z"
        },
        "trusted": true,
        "id": "eqHwHFPtQzkp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is an interesting finding that there is a strong positive correlation between \"SkinThickness\" and \"Insulin\" variables' missing values. It may be used later.\n",
        "\n",
        "\"missingno\" library helps us to visualize the missing values, now let's more mathematical approach. Let's make a table that consists of missing values. For this, write a function that shows the missing values on a dataframe as a table."
      ],
      "metadata": {
        "id": "IfPPPEipQzkt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def missing_values_table(dataframe, na_name=False):\n",
        "    # only take missing columns\n",
        "    na_columns = [col for col in dataframe.columns if dataframe[col].isnull().sum() > 0]\n",
        "\n",
        "    n_miss = dataframe[na_columns].isnull().sum().sort_values(ascending=False)  # number of missing value\n",
        "    ratio = (dataframe[na_columns].isnull().sum() / dataframe.shape[0] * 100).sort_values(ascending=False) # ratio\n",
        "    missing_df = pd.concat([n_miss, np.round(ratio, 2)], axis=1, keys=['n_miss', 'ratio'])  # make table\n",
        "    print(missing_df, end=\"\\n\")\n",
        "\n",
        "    if na_name:\n",
        "        return na_columns"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-02T16:08:16.115939Z",
          "iopub.execute_input": "2022-08-02T16:08:16.116478Z",
          "iopub.status.idle": "2022-08-02T16:08:16.125936Z",
          "shell.execute_reply.started": "2022-08-02T16:08:16.116427Z",
          "shell.execute_reply": "2022-08-02T16:08:16.124187Z"
        },
        "trusted": true,
        "id": "m4FPbNKyQzku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "missing_values_table(df)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-02T16:08:16.12743Z",
          "iopub.execute_input": "2022-08-02T16:08:16.128398Z",
          "iopub.status.idle": "2022-08-02T16:08:16.155499Z",
          "shell.execute_reply.started": "2022-08-02T16:08:16.12835Z",
          "shell.execute_reply": "2022-08-02T16:08:16.153488Z"
        },
        "trusted": true,
        "id": "1s9jpQ30Qzku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# if column names wants to be stored, na_name argument can be made True\n",
        "na_cols = missing_values_table(df, na_name=True)\n",
        "na_cols"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-02T16:08:16.157547Z",
          "iopub.execute_input": "2022-08-02T16:08:16.158111Z",
          "iopub.status.idle": "2022-08-02T16:08:16.182385Z",
          "shell.execute_reply.started": "2022-08-02T16:08:16.158058Z",
          "shell.execute_reply": "2022-08-02T16:08:16.180212Z"
        },
        "trusted": true,
        "id": "Y5uE-yqlQzku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's create another table that relation between target variable(Outcome) and another variables."
      ],
      "metadata": {
        "id": "pKnU1hVdQzku"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We created na_cols above, it can be taken with similar way in function:\n",
        "# na_columns = [i for i in df.columns if df[i].isnull().sum() > 0]\n",
        "def missing_vs_target(dataframe, target, na_columns):\n",
        "    temp_df = dataframe.copy()\n",
        "\n",
        "    for col in na_columns:\n",
        "        temp_df[col + '_NA_FLAG'] = np.where(temp_df[col].isnull(), 1, 0)  # eksiklik varsa 1, yoksa 0 yaz\n",
        "\n",
        "    # [tüm satırları getir, sütunlarda içinde NA olanları getir]\n",
        "    na_flags = temp_df.loc[:, temp_df.columns.str.contains(\"_NA_\")].columns\n",
        "\n",
        "    for col in na_flags:\n",
        "        print(pd.DataFrame({\"TARGET_MEAN\": temp_df.groupby(col)[target].mean(),\n",
        "                            \"Count\": temp_df.groupby(col)[target].count()}), end=\"\\n\\n\\n\")\n",
        "\n",
        "\n",
        "missing_vs_target(df, \"Outcome\", na_cols)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-02T16:08:16.185555Z",
          "iopub.execute_input": "2022-08-02T16:08:16.186774Z",
          "iopub.status.idle": "2022-08-02T16:08:16.238361Z",
          "shell.execute_reply.started": "2022-08-02T16:08:16.186717Z",
          "shell.execute_reply": "2022-08-02T16:08:16.236835Z"
        },
        "trusted": true,
        "id": "7ksj5upDQzku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "On NA_FLAG variables, the missing values are filled with 1, and 0 where there is no missing value. For instance, in Glucose, there are 763 non-missing values and 5 missing values. The mean Outcome for non-missing values is 0.349, and the mean of Outcome for missing values is 0.4.\n",
        "\n",
        "Why this is significant is because of observation. For example, we may drop \"Insulin\" because almost half of it missing, but when the relationship between the target is investigated, the mean of disease is much more than non-missing values. The same situation can be observed on SkinThickness. Remember, we found the missing correlation between Insulin and SkinThickness, so that, my opinion is not to drop these at that point.\n",
        "\n",
        "The correlation is another approach to look for inversing whether there is a relation."
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-02T02:40:03.258494Z",
          "iopub.execute_input": "2022-08-02T02:40:03.258866Z",
          "iopub.status.idle": "2022-08-02T02:40:03.271061Z",
          "shell.execute_reply.started": "2022-08-02T02:40:03.258834Z",
          "shell.execute_reply": "2022-08-02T02:40:03.269622Z"
        },
        "id": "bey66ByVQzku"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_cor = pd.DataFrame([df[\"Outcome\"].corr(df[i]) for i in num_cols], index=num_cols, columns=[\"Correlation\"])\n",
        "df_cor"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-02T16:08:16.244299Z",
          "iopub.execute_input": "2022-08-02T16:08:16.24554Z",
          "iopub.status.idle": "2022-08-02T16:08:16.270338Z",
          "shell.execute_reply.started": "2022-08-02T16:08:16.245499Z",
          "shell.execute_reply": "2022-08-02T16:08:16.268042Z"
        },
        "trusted": true,
        "id": "rBE-FYmSQzku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The most considerable variable is Glucose, but Insuline may not be ignored even if it has too many missing values because there is a slight correlation, as well.\n",
        "\n",
        "With this analysis, EDA part is completed."
      ],
      "metadata": {
        "id": "irPAJp_yQzku"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"3\"></a> <br>\n",
        "# 3. Pre-processing (Solve Outliers and Missing Values)\n",
        "\n",
        "In part 2, the analysis has been completed. Examination of thresholds for outliers and how to reach missing values have been done. The next step is to interfere with these and prepare the dataset. This part will be separated into 2, like part 2. Firstly, the outliers problem will be examined, then, missing values will be processed.\n",
        "\n",
        "Our threshold values can be affected after the operations on the missing value. Also, an outlier method, LOF, cannot be used when the dataset includes NaN values. Therefore, the missing value part will be taken firstly on preprocessing.\n",
        "\n",
        "### **Missing Values**\n",
        "\n",
        "It mentioned on analyze part that there is 3 solution to missing values.\n",
        "\n",
        "- **Deleting:** It means dropping the rows that include missing values. Especially in a small dataset, it creates a loss of information. For example, the dataset has 768 rows, but Insuline has 374 missing values. If missing values are dropped, half of the dataset has been lost, and the information will be lost, as well. If the dataset would be large, and there are a couple of missing values that can be sacrificed, deleting can be an option."
      ],
      "metadata": {
        "id": "lRSQz2q6Qzku"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_new = df.copy() # copy dataset to see effect without damage the main dataset\n",
        "df_new.shape"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-02T16:08:16.272641Z",
          "iopub.execute_input": "2022-08-02T16:08:16.273855Z",
          "iopub.status.idle": "2022-08-02T16:08:16.287656Z",
          "shell.execute_reply.started": "2022-08-02T16:08:16.2738Z",
          "shell.execute_reply": "2022-08-02T16:08:16.285957Z"
        },
        "trusted": true,
        "id": "JEmGbAj2Qzku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_new.dropna(inplace=True)\n",
        "df_new.shape"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-02T16:08:16.290375Z",
          "iopub.execute_input": "2022-08-02T16:08:16.291402Z",
          "iopub.status.idle": "2022-08-02T16:08:16.305201Z",
          "shell.execute_reply.started": "2022-08-02T16:08:16.291334Z",
          "shell.execute_reply": "2022-08-02T16:08:16.303332Z"
        },
        "trusted": true,
        "id": "oz4rVaNKQzku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Value Assignment Methods**: We can fill the NaN values with the column's mean, median, mode, etc. If the distribution is homogeneous, filling with median or mean is logical. Also, in some scenarios, filling with a specific number like 0 would make sense."
      ],
      "metadata": {
        "id": "l7leenH4Qzku"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_fill = df.apply(lambda x: x.fillna(x.median()) if x.dtype != \"O\" else x, axis=0)\n",
        "df_fill.head(10) # after filling"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-02T16:08:16.306752Z",
          "iopub.execute_input": "2022-08-02T16:08:16.307631Z",
          "iopub.status.idle": "2022-08-02T16:08:16.335281Z",
          "shell.execute_reply.started": "2022-08-02T16:08:16.307594Z",
          "shell.execute_reply": "2022-08-02T16:08:16.333723Z"
        },
        "trusted": true,
        "id": "tZswS_6HQzku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# before filling\n",
        "df.head(10)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-02T16:08:16.33745Z",
          "iopub.execute_input": "2022-08-02T16:08:16.339035Z",
          "iopub.status.idle": "2022-08-02T16:08:16.359571Z",
          "shell.execute_reply.started": "2022-08-02T16:08:16.338979Z",
          "shell.execute_reply": "2022-08-02T16:08:16.358514Z"
        },
        "trusted": true,
        "id": "2v3HtpqPQzkv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Predictive Methods:** This method is based on machine learning, statistical methods, etc. It is an advanced level to fill NaN values. A model can be implemented and missing values are predicted by that model in this method. There are 2 points we have to consider:\n",
        "\n",
        ">1. We have to put categorical variables into a one-hot encoder. Since we will use a modeling technique, the model expects variables from us in some specific ways, and we have to comply with these. <br>\n",
        "2. Since KNN is a distance-based algorithm, we need to standardize the variables.\n",
        "\n",
        "Encoding and standardization will be explained in the next part, so that, only the predictive method process will be shown in this part. The only point, for now, since all variables are numerical, encoding will not be done."
      ],
      "metadata": {
        "id": "daXrobH7Qzkv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_ml = df.copy()\n",
        "\n",
        "# standardization\n",
        "scaler = MinMaxScaler()\n",
        "df_ml = pd.DataFrame(scaler.fit_transform(df_ml), columns=df_ml.columns)\n",
        "df_ml.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-02T16:08:16.362868Z",
          "iopub.execute_input": "2022-08-02T16:08:16.364025Z",
          "iopub.status.idle": "2022-08-02T16:08:16.392124Z",
          "shell.execute_reply.started": "2022-08-02T16:08:16.363971Z",
          "shell.execute_reply": "2022-08-02T16:08:16.390601Z"
        },
        "trusted": true,
        "id": "TQnzHtuXQzkv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fill with KNN\n",
        "from sklearn.impute import KNNImputer\n",
        "imputer = KNNImputer(n_neighbors=5)\n",
        "df_ml = pd.DataFrame(imputer.fit_transform(df_ml), columns=df_ml.columns)\n",
        "df_ml.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-02T16:08:16.393609Z",
          "iopub.execute_input": "2022-08-02T16:08:16.39406Z",
          "iopub.status.idle": "2022-08-02T16:08:16.488645Z",
          "shell.execute_reply.started": "2022-08-02T16:08:16.394027Z",
          "shell.execute_reply": "2022-08-02T16:08:16.486722Z"
        },
        "trusted": true,
        "id": "QYxYJnUrQzkv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this way, we have completed the filling process, but the problem is all values we filled become standardized. It can turn the normal values with \"inverse_transform\"."
      ],
      "metadata": {
        "id": "A8dSHtW4Qzkv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_ml = pd.DataFrame(scaler.inverse_transform(df_ml), columns=df_ml.columns)\n",
        "df_ml.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-02T16:08:16.491588Z",
          "iopub.execute_input": "2022-08-02T16:08:16.492806Z",
          "iopub.status.idle": "2022-08-02T16:08:16.529041Z",
          "shell.execute_reply.started": "2022-08-02T16:08:16.492733Z",
          "shell.execute_reply": "2022-08-02T16:08:16.527013Z"
        },
        "trusted": true,
        "id": "qAv36Xz5Qzkv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# our first dataset\n",
        "df.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-02T16:08:16.532005Z",
          "iopub.execute_input": "2022-08-02T16:08:16.533189Z",
          "iopub.status.idle": "2022-08-02T16:08:16.549894Z",
          "shell.execute_reply.started": "2022-08-02T16:08:16.533118Z",
          "shell.execute_reply": "2022-08-02T16:08:16.548793Z"
        },
        "trusted": true,
        "id": "Oev8ClImQzkv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We examined the possible ways to deal with missing data. In order to examine the ways, the methods have been implemented in different data frames. Now, we have to apply these methods to the main dataframe.\n",
        "\n",
        "Firstly, check again the number of missing values with the function we defined on analyze part."
      ],
      "metadata": {
        "id": "Z7SpezKlQzkv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "missing_values_table(df)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-02T16:08:16.551268Z",
          "iopub.execute_input": "2022-08-02T16:08:16.551813Z",
          "iopub.status.idle": "2022-08-02T16:08:16.568657Z",
          "shell.execute_reply.started": "2022-08-02T16:08:16.55178Z",
          "shell.execute_reply": "2022-08-02T16:08:16.567464Z"
        },
        "trusted": true,
        "id": "Jdw8HIeYQzkv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, my approach will separate the process into 2. Since two variables have many missing values, making a prediction may not be logical for these. It is just an interpretation and this is open to comment. The data preprocessing part is subjective, but the common aim is to prepare data as clearly as possible before the model.\n",
        "\n",
        "My comment is to use predict model for variables that include a low number of missing values and fill median for variables that have a high number of missing."
      ],
      "metadata": {
        "id": "jy215XNxQzkv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "na_cols = missing_values_table(df, na_name=True) # columns that includes missing values\n",
        "n_miss = df[na_cols].isnull().sum() # number of missing values on variables\n",
        "\n",
        "# 100 as a threshold, it is open to comment\n",
        "na_cols_ml = [i for i in n_miss.index if n_miss[i] < 100]\n",
        "na_cols_med = [i for i in n_miss.index if n_miss[i] > 100]\n",
        "print(\"Columns that will be applied ML model:\", na_cols_ml)\n",
        "print(\"Columns that will be filled with median:\", na_cols_med)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-02T16:08:16.569869Z",
          "iopub.execute_input": "2022-08-02T16:08:16.570238Z",
          "iopub.status.idle": "2022-08-02T16:08:16.591793Z",
          "shell.execute_reply.started": "2022-08-02T16:08:16.570205Z",
          "shell.execute_reply": "2022-08-02T16:08:16.590346Z"
        },
        "trusted": true,
        "id": "kVm5vUPvQzkv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check before process\n",
        "df.head(10)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-02T16:08:16.593513Z",
          "iopub.execute_input": "2022-08-02T16:08:16.594533Z",
          "iopub.status.idle": "2022-08-02T16:08:16.610847Z",
          "shell.execute_reply.started": "2022-08-02T16:08:16.594496Z",
          "shell.execute_reply": "2022-08-02T16:08:16.6091Z"
        },
        "trusted": true,
        "id": "pDXhUW4cQzkv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for the number of missing value is less than 100\n",
        "df[na_cols_med] = df[na_cols_med].apply(lambda x: x.fillna(x.median()) if x.dtype != \"O\" else x, axis=0)\n",
        "df.head(10)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-02T16:08:16.612414Z",
          "iopub.execute_input": "2022-08-02T16:08:16.612812Z",
          "iopub.status.idle": "2022-08-02T16:08:16.634628Z",
          "shell.execute_reply.started": "2022-08-02T16:08:16.612779Z",
          "shell.execute_reply": "2022-08-02T16:08:16.633322Z"
        },
        "trusted": true,
        "id": "SE66XADMQzkw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As it can be seen above, \"SkinThickness\" and \"Insulin\" have been filled with their median values. Now, variables that have slight number of missing values will be filled with ML model."
      ],
      "metadata": {
        "id": "3lGSCkqIQzkw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# standardization\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# take only needed columns\n",
        "df[na_cols_ml] = pd.DataFrame(scaler.fit_transform(df[na_cols_ml]), columns=df[na_cols_ml].columns)\n",
        "print(df[na_cols_ml].head())\n",
        "\n",
        "# fill with KNN\n",
        "from sklearn.impute import KNNImputer\n",
        "imputer = KNNImputer(n_neighbors=5)\n",
        "df[na_cols_ml] = pd.DataFrame(imputer.fit_transform(df[na_cols_ml]), columns=df[na_cols_ml].columns)\n",
        "print(df[na_cols_ml].head())\n",
        "\n",
        "# from standardized to non-standardized\n",
        "df[na_cols_ml] = pd.DataFrame(scaler.inverse_transform(df[na_cols_ml]), columns=df[na_cols_ml].columns)\n",
        "print(df[na_cols_ml].head())"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-02T16:08:16.635985Z",
          "iopub.execute_input": "2022-08-02T16:08:16.637016Z",
          "iopub.status.idle": "2022-08-02T16:08:16.675304Z",
          "shell.execute_reply.started": "2022-08-02T16:08:16.636982Z",
          "shell.execute_reply": "2022-08-02T16:08:16.673841Z"
        },
        "trusted": true,
        "id": "Uj4oBzbNQzkw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(10)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-02T16:08:16.677349Z",
          "iopub.execute_input": "2022-08-02T16:08:16.677708Z",
          "iopub.status.idle": "2022-08-02T16:08:16.694136Z",
          "shell.execute_reply.started": "2022-08-02T16:08:16.677674Z",
          "shell.execute_reply": "2022-08-02T16:08:16.692795Z"
        },
        "trusted": true,
        "id": "K-ljLkfgQzkw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As it can be seen, NaN values are filled. For the last time, check the number of missing values."
      ],
      "metadata": {
        "id": "wCTkglP3Qzkw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-02T16:08:16.702321Z",
          "iopub.execute_input": "2022-08-02T16:08:16.703165Z",
          "iopub.status.idle": "2022-08-02T16:08:16.713576Z",
          "shell.execute_reply.started": "2022-08-02T16:08:16.703128Z",
          "shell.execute_reply": "2022-08-02T16:08:16.712111Z"
        },
        "trusted": true,
        "id": "d6x65NSbQzkw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Outliers**\n",
        "\n",
        "We mentioned on analyze part that there are 2 ways to solve this problem: <ins>IQR Method</ins> for the find outliers on univariate, and <ins>LOF Method</ins> for getting multivariate outliers.\n",
        "\n",
        "Firstly, LOF Method will be explained because it controls the variables' meaningfulness to each other and gives a few outliers. Then, IQR Method will be implemented.\n",
        "\n",
        "\n",
        "* **Local Outlier Factor (LOF):** Another approach is the Local Outlier Factor. It helps us to define outliers accordingly by ordering the observations based on the density at their location. The local density of a point means the neighborhoods around that point. If a point is significantly less dense than its neighbors, then that point is in a more sparse region, so there may be an outlier. The LOF method allows us to calculate a distance score based on neighborhoods.\n",
        "\n",
        "![LOF.png](attachment:ed391e8f-cc8e-4179-a64f-74516d887fd3.png)\n",
        "\n",
        "The LOF method says: \"I'll give you a score, the closer the score to 1 the better\". Therefore, as you move away from 1, the probability of the observation being outlier increases. The LOF method is also used to determine thresholds.\n",
        "\n",
        "**Example of usage:** In our dataset, there are both \"Age\" and \"Pregnancies\". For instance, when these variables are taken separately, there may not be any outlier. However, when these 2 variables get together, it may be the outlier. What it means is explained in the following example dataframe."
      ],
      "metadata": {
        "id": "xBy_b-MPQzkw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_ex = pd.DataFrame({\"age\": [17, 35, 47],\n",
        "                     \"pregnancy\": [5, 2, 3]})\n",
        "df_ex"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-02T16:08:16.714918Z",
          "iopub.execute_input": "2022-08-02T16:08:16.715875Z",
          "iopub.status.idle": "2022-08-02T16:08:16.731759Z",
          "shell.execute_reply.started": "2022-08-02T16:08:16.715839Z",
          "shell.execute_reply": "2022-08-02T16:08:16.729977Z"
        },
        "trusted": true,
        "id": "4N4g65F5Qzkw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When we take the variables separately, \"Age\" is looking fine. A person can be 17, 35, or 47 years old. Similarly, \"Pregnancy\" has no problem if we consider only the number of pregnancies because a person can be pregnant 5 times, twice, or 3 times.\n",
        "\n",
        "Now, check the first index, the values have meanings separately, but together, a person 17 years old cannot be pregnant 5 times. This is an outlier row. LOF helps us to find these kinds of values and fix these.\n",
        "\n",
        "The LOF scores will be considered, then, <ins>\"elbow method\"</ins> will be implemented to determine threshold."
      ],
      "metadata": {
        "id": "M-DY6P52Qzkw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# LOF\n",
        "clf = LocalOutlierFactor(n_neighbors=20)\n",
        "clf.fit_predict(df)  # returns LOF scores\n",
        "df_scores = clf.negative_outlier_factor_ # keep scores to observe (negative)\n",
        "# df_scores = -df_scores # for changing to pozitive but we will use as negative\n",
        "print(df_scores[0:5])\n",
        "print(np.sort(df_scores)[0:5])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-02T16:08:16.734491Z",
          "iopub.execute_input": "2022-08-02T16:08:16.735073Z",
          "iopub.status.idle": "2022-08-02T16:08:16.7583Z",
          "shell.execute_reply.started": "2022-08-02T16:08:16.73502Z",
          "shell.execute_reply": "2022-08-02T16:08:16.757039Z"
        },
        "trusted": true,
        "id": "l5vVFEweQzkw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NOTE:** \"negative_outlier_factor_\" method gives scores as negative. These can be changed to positive but the original negative values will be used in order to observe it more easily in the elbow method that will be used. Therefore, if scores are close to -1, the result becomes not an outlier, not being close to 1 like at the beginning. For instance, if we have scores from -1 to -10, it will be interpreted that values at -10 tend to be more outlier."
      ],
      "metadata": {
        "id": "i0xYXEB9Qzkw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# elbow method\n",
        "scores = pd.DataFrame(np.sort(df_scores))\n",
        "scores.plot(stacked=True, xlim=[0, 20], style='.-')  # x=gözlemler, y=outlier skorları\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-02T16:08:16.760038Z",
          "iopub.execute_input": "2022-08-02T16:08:16.760826Z",
          "iopub.status.idle": "2022-08-02T16:08:17.026726Z",
          "shell.execute_reply.started": "2022-08-02T16:08:16.760787Z",
          "shell.execute_reply": "2022-08-02T16:08:17.02569Z"
        },
        "trusted": true,
        "id": "vkJNqsP3Qzkw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are observations on the x-axis and outlier scores on the y-axis. Each point represents threshold values and a graph was created according to these threshold values. The threshold value has to be decided according to the table. The point where the steepest slope change is the 3rd index. Hence, the final comment can be:\n",
        "\n",
        "There are some observations and their scores, and there is the problem of where to divide these scores. The most marginal change was in the 3rd index, so this is the point where the most drastic change is defined as the threshold value. Because the lower scores(more negative values) are the worse, then a threshold has to be set and cut it."
      ],
      "metadata": {
        "id": "apK1JotBQzkw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "th = np.sort(df_scores)[3]  # set any lower scores than that as outlier\n",
        "df[df_scores < th] # check outliers"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-02T16:08:17.028092Z",
          "iopub.execute_input": "2022-08-02T16:08:17.028628Z",
          "iopub.status.idle": "2022-08-02T16:08:17.043514Z",
          "shell.execute_reply.started": "2022-08-02T16:08:17.028596Z",
          "shell.execute_reply": "2022-08-02T16:08:17.042289Z"
        },
        "trusted": true,
        "id": "51uG_EVdQzkw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe([0.01, 0.05, 0.25, 0.50, 0.75, 0.90, 0.99]).T"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-02T16:08:17.045325Z",
          "iopub.execute_input": "2022-08-02T16:08:17.045759Z",
          "iopub.status.idle": "2022-08-02T16:08:17.098605Z",
          "shell.execute_reply.started": "2022-08-02T16:08:17.045717Z",
          "shell.execute_reply": "2022-08-02T16:08:17.097075Z"
        },
        "trusted": true,
        "id": "un13HzwxQzkw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For instance, for the 13th index, while SkinThickness is close to 25% quantile, Insulin is max. On the 445th index, maybe when Glucose and BMI are close to high values, but Insuline is minimum, and this kind of combination may not be possible.\n",
        "\n",
        "Being close(or be exactly) to max or min values are not obligatory. These are just comments without any industry knowledge, maybe these are wrong but these kinds of comments have to be done for understanding the data.\n",
        "\n",
        "There is just 3 observation on the data set, so they can be deleted."
      ],
      "metadata": {
        "id": "4aAOio4UQzkx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Before delete outliers:\", df.shape)\n",
        "print(df[df_scores < th].index) # indexes of outliers, just for observation\n",
        "df.drop(axis=0, labels=df[df_scores < th].index, inplace=True)\n",
        "print(\"After delete outliers:\", df.shape)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-02T16:08:17.1002Z",
          "iopub.execute_input": "2022-08-02T16:08:17.100604Z",
          "iopub.status.idle": "2022-08-02T16:08:17.111942Z",
          "shell.execute_reply.started": "2022-08-02T16:08:17.10057Z",
          "shell.execute_reply": "2022-08-02T16:08:17.110574Z"
        },
        "trusted": true,
        "id": "9_gMWLzzQzkx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **IQR:** We find the thresholds on analysis using IQR calculation. Now, re-assignment with thresholds will be done. It means the outliers will be replaced with upper and lower thresholds. Why this kind of process is made is based on preventing the loss of data. Especially in the small dataset, each data is significant. <br>\n",
        "<br>\n",
        "In the literature, the upper limit is taken as 75% quantile(Q3), and a higher value than 1.5xQ3 will be replaced this threshold, a similar process is done with the lower value that is lower than 1.5xQ1, where Q1 is 25% quantile. We can also use quantiles as 5 by 95 because deleting or filling according to 25-75 would create a serious data loss and residue, we would add noise ourselves and create problems. Hence, the proportion of these depends according to the problems and datasets. If tree methods are used, not to touch outliers may be chosen, or approaching unacceptable outliers a shave from the end may be implemented.\n",
        "\n",
        "In the analysis part, thresholds have been found with the function \"outlier_thresholds\". Now, write a function that takes these thresholds and makes re-assignment to outliers."
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-02T11:00:25.41797Z",
          "iopub.execute_input": "2022-08-02T11:00:25.418426Z",
          "iopub.status.idle": "2022-08-02T11:00:25.43373Z",
          "shell.execute_reply.started": "2022-08-02T11:00:25.418385Z",
          "shell.execute_reply": "2022-08-02T11:00:25.432755Z"
        },
        "id": "_HOxXW1OQzkx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def replace_with_thresholds(dataframe, variable):\n",
        "    low_limit, up_limit = outlier_thresholds(dataframe, variable)\n",
        "    dataframe.loc[(dataframe[variable] < low_limit), variable] = low_limit\n",
        "    dataframe.loc[(dataframe[variable] > up_limit), variable] = up_limit"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-02T16:08:17.113229Z",
          "iopub.execute_input": "2022-08-02T16:08:17.113755Z",
          "iopub.status.idle": "2022-08-02T16:08:17.122763Z",
          "shell.execute_reply.started": "2022-08-02T16:08:17.113692Z",
          "shell.execute_reply": "2022-08-02T16:08:17.121349Z"
        },
        "trusted": true,
        "id": "FQctofbyQzkx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# before re-assignment\n",
        "df.describe().T"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-02T16:08:17.12456Z",
          "iopub.execute_input": "2022-08-02T16:08:17.124973Z",
          "iopub.status.idle": "2022-08-02T16:08:17.173643Z",
          "shell.execute_reply.started": "2022-08-02T16:08:17.124907Z",
          "shell.execute_reply": "2022-08-02T16:08:17.172343Z"
        },
        "trusted": true,
        "id": "IHdMZ8T0Qzkx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for col in num_cols:\n",
        "    replace_with_thresholds(df, col)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-02T16:08:17.176467Z",
          "iopub.execute_input": "2022-08-02T16:08:17.177029Z",
          "iopub.status.idle": "2022-08-02T16:08:17.217211Z",
          "shell.execute_reply.started": "2022-08-02T16:08:17.176977Z",
          "shell.execute_reply": "2022-08-02T16:08:17.215667Z"
        },
        "trusted": true,
        "id": "WyCySznKQzkx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# after re-assignment\n",
        "df.describe().T"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-02T16:08:17.219069Z",
          "iopub.execute_input": "2022-08-02T16:08:17.219473Z",
          "iopub.status.idle": "2022-08-02T16:08:17.267605Z",
          "shell.execute_reply.started": "2022-08-02T16:08:17.219438Z",
          "shell.execute_reply": "2022-08-02T16:08:17.266236Z"
        },
        "trusted": true,
        "id": "e4-ECBKwQzkx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As it can be seen, min, max, event mean, and std values have changed according to thresholds. Solution of outliers has been completed with this process."
      ],
      "metadata": {
        "id": "I4Il_Qi4Qzkx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"4\"></a> <br>\n",
        "# 4. Feature Extraction\n",
        "\n",
        "When a data set is prepared, not only existing variables are tried to be edited, but also new, meaningful variables have to be created. New columns sometimes can be created with mathematical operations, sometimes named a numerical value to categorical, or categorical values' ranges, etc. This process is known as **Feature Engineering**, and this is one of the critical parts of data preparation."
      ],
      "metadata": {
        "id": "sxeYFVoKQzkx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pregrancy cannot be float, it occurs due to calculation of IQR\n",
        "df[\"Pregnancies\"] = df[\"Pregnancies\"].apply(lambda x: int(x))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-02T16:08:17.26992Z",
          "iopub.execute_input": "2022-08-02T16:08:17.270376Z",
          "iopub.status.idle": "2022-08-02T16:08:17.278733Z",
          "shell.execute_reply.started": "2022-08-02T16:08:17.27034Z",
          "shell.execute_reply": "2022-08-02T16:08:17.277025Z"
        },
        "trusted": true,
        "id": "45VtOlDBQzky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create categorical columns from numerical columns\n",
        "\n",
        "# if bins are 0, 3, 6 => 0 values become NaN due to bins\n",
        "df[\"NumOfPreg\"] = pd.cut(df[\"Pregnancies\"], bins=[-1, 3, 6, df[\"Pregnancies\"].max()], labels=[\"Normal\", \"Above Normal\",\"Extreme\"])\n",
        "df[\"AgeGroup\"] = pd.cut(df[\"Age\"], bins=[18, 25, 40, df[\"Age\"].max()], labels=[\"Young\", \"Mature\", \"Old\"])\n",
        "df[\"GlucoseGroup\"] = pd.qcut(df[\"Glucose\"], 3, labels=[\"Low\", \"Medium\", \"High\"])\n",
        "df[\"Patient\"] = np.where(df[\"Outcome\"] == 1, \"Yes\", \"No\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-02T16:08:17.281182Z",
          "iopub.execute_input": "2022-08-02T16:08:17.281643Z",
          "iopub.status.idle": "2022-08-02T16:08:17.299867Z",
          "shell.execute_reply.started": "2022-08-02T16:08:17.281608Z",
          "shell.execute_reply": "2022-08-02T16:08:17.298173Z"
        },
        "trusted": true,
        "id": "UwS3ZDwNQzky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# example of mathematical expression\n",
        "\n",
        "\"\"\"Assume there is a variable named \"BMIns\", and it can be found with the multiplication of BMI and Insuline.\n",
        "Create and add it to data frame\"\"\"\n",
        "\n",
        "df[\"BMIns\"] = df[\"BMI\"]*df[\"Insulin\"] # numerical\n",
        "df[\"BMInsGroup\"] = pd.qcut(df[\"BMIns\"], 3, labels=[\"Low\", \"Medium\", \"High\"]) # categorical"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-02T16:08:17.301683Z",
          "iopub.execute_input": "2022-08-02T16:08:17.302043Z",
          "iopub.status.idle": "2022-08-02T16:08:17.316292Z",
          "shell.execute_reply.started": "2022-08-02T16:08:17.30201Z",
          "shell.execute_reply": "2022-08-02T16:08:17.314535Z"
        },
        "trusted": true,
        "id": "EMgpR6HIQzky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-02T16:08:17.318027Z",
          "iopub.execute_input": "2022-08-02T16:08:17.318424Z",
          "iopub.status.idle": "2022-08-02T16:08:17.344969Z",
          "shell.execute_reply.started": "2022-08-02T16:08:17.31839Z",
          "shell.execute_reply": "2022-08-02T16:08:17.343389Z"
        },
        "trusted": true,
        "id": "erUJ-BPZQzky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"5\"></a> <br>\n",
        "# 5. Encoding & Scaling"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-02T11:47:26.37641Z",
          "iopub.execute_input": "2022-08-02T11:47:26.376869Z",
          "iopub.status.idle": "2022-08-02T11:47:26.419127Z",
          "shell.execute_reply.started": "2022-08-02T11:47:26.376815Z",
          "shell.execute_reply": "2022-08-02T11:47:26.417696Z"
        },
        "id": "5Op_xZ42Qzky"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Encoding**\n",
        "Changing the representation of variables. There are different types of encoding:\n",
        "> If there are 2 classes, it is called **\"Binary Encoding\"**, if there are more than 2 classes, it is called **\"Label Encoding\"**.\n",
        "\n",
        "It converts in alphabetical order, giving 0 to the first one it sees."
      ],
      "metadata": {
        "id": "cL-bloMVQzky"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_enc = df.copy()\n",
        "\n",
        "le = LabelEncoder()\n",
        "le.fit_transform(df_enc[\"Patient\"])[0:5]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-02T16:08:17.34663Z",
          "iopub.execute_input": "2022-08-02T16:08:17.347095Z",
          "iopub.status.idle": "2022-08-02T16:08:17.360019Z",
          "shell.execute_reply.started": "2022-08-02T16:08:17.34706Z",
          "shell.execute_reply": "2022-08-02T16:08:17.358527Z"
        },
        "trusted": true,
        "id": "jruWHrqkQzky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's say we forgot which 0 and which 1, inverse_transform is used to detect this\n",
        "le.inverse_transform([0, 1])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-02T16:08:17.361697Z",
          "iopub.execute_input": "2022-08-02T16:08:17.362403Z",
          "iopub.status.idle": "2022-08-02T16:08:17.370938Z",
          "shell.execute_reply.started": "2022-08-02T16:08:17.362322Z",
          "shell.execute_reply": "2022-08-02T16:08:17.369648Z"
        },
        "trusted": true,
        "id": "1FeyPB9AQzky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# detect variables that have 2 unique numbers for Binary Encoding\n",
        "binary_cols = [col for col in df.columns if df[col].dtype not in [int, float]\n",
        "               and df[col].nunique() == 2]\n",
        "binary_cols"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-02T16:08:17.373481Z",
          "iopub.execute_input": "2022-08-02T16:08:17.373969Z",
          "iopub.status.idle": "2022-08-02T16:08:17.390748Z",
          "shell.execute_reply.started": "2022-08-02T16:08:17.373935Z",
          "shell.execute_reply": "2022-08-02T16:08:17.389412Z"
        },
        "trusted": true,
        "id": "EZSK1-RJQzky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If a variable is converted like this to numeric values and sent to algorithms, the algorithm defines this variable as a variable ranging from an interval. In other words, this transformation will confuse the algorithms. The reason for its corruption is here, the categorical variable has turned into numerical values, and a categorical variable, which is normally a nominal scale type, is suddenly distorted by a transformation process and is subjected to a sorting process as if there is a difference between them. The characteristic of the nominal scale type is that the distance between classes is equal, but this equality is disturbed after this transformation.\n",
        "\n",
        "This situation can cause biases and problems when we convert categorical variables to numeric values and confuse algorithms. In this case, we need to do **one-hot encoding**.\n",
        "\n",
        "To fix this problem, one-hot conversion is done, but doing this also creates a trap named as \"dummy variable trap.\"\n",
        "\n",
        "If we apply a transformation to the variables in the data set, if the new variables can be created over each other, this is a dummy variable trap. In other words, if there is a variable that represents another variable, this situation is called a dummy variable trap.\n",
        "\n",
        "Let's see how one-hot encoding is done, what the dummy variable trap is, and how to fix that."
      ],
      "metadata": {
        "id": "FxsCX6tYQzky"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-02T16:08:17.392582Z",
          "iopub.execute_input": "2022-08-02T16:08:17.393657Z",
          "iopub.status.idle": "2022-08-02T16:08:17.415483Z",
          "shell.execute_reply.started": "2022-08-02T16:08:17.393621Z",
          "shell.execute_reply": "2022-08-02T16:08:17.414431Z"
        },
        "trusted": true,
        "id": "yQ2ll0fAQzky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_new = pd.get_dummies(df, columns=[\"Patient\"])\n",
        "df_new.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-02T16:08:17.417404Z",
          "iopub.execute_input": "2022-08-02T16:08:17.417854Z",
          "iopub.status.idle": "2022-08-02T16:08:17.448078Z",
          "shell.execute_reply.started": "2022-08-02T16:08:17.417795Z",
          "shell.execute_reply": "2022-08-02T16:08:17.446619Z"
        },
        "trusted": true,
        "id": "mbAYjGB4Qzky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As it can be seen, Patient is dropped, and there are new variables: Patient_No and Patient_Yes. The problem is Patient_No is the reverse of the Patient_Yes, which means, Patient_No can be created over Patient_Yes. These both carry the same information, and both are on a data frame. In that situation, we have to drop one of these, and it helps to avoid creating over each other and duplication. The dummy variable trap is solved with this."
      ],
      "metadata": {
        "id": "CZJng9AIQzky"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_new = pd.get_dummies(df, columns=[\"Patient\"], drop_first=True)\n",
        "df_new.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-02T16:08:17.454819Z",
          "iopub.execute_input": "2022-08-02T16:08:17.455232Z",
          "iopub.status.idle": "2022-08-02T16:08:17.48345Z",
          "shell.execute_reply.started": "2022-08-02T16:08:17.455198Z",
          "shell.execute_reply": "2022-08-02T16:08:17.481935Z"
        },
        "trusted": true,
        "id": "Gl0WYTEoQzky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Also, a prefix of the new variables can be defined as an argument."
      ],
      "metadata": {
        "id": "_kztr5_8Qzky"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_new = pd.get_dummies(df, columns=[\"Patient\"], drop_first=True, prefix=[\"Sick\"])\n",
        "df_new.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-02T16:08:17.484798Z",
          "iopub.execute_input": "2022-08-02T16:08:17.486018Z",
          "iopub.status.idle": "2022-08-02T16:08:17.511833Z",
          "shell.execute_reply.started": "2022-08-02T16:08:17.485978Z",
          "shell.execute_reply": "2022-08-02T16:08:17.510516Z"
        },
        "trusted": true,
        "id": "H69PsYurQzkz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, apply this method to our main dataset's categorical variables. Firstly, take categorical variables again with the function we write in the analysis part."
      ],
      "metadata": {
        "id": "92Gj19NeQzkz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cat_cols, num_cols, cat_but_car = grab_col_names(df)\n",
        "cat_cols"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-02T16:08:17.513794Z",
          "iopub.execute_input": "2022-08-02T16:08:17.514339Z",
          "iopub.status.idle": "2022-08-02T16:08:17.532739Z",
          "shell.execute_reply.started": "2022-08-02T16:08:17.514287Z",
          "shell.execute_reply": "2022-08-02T16:08:17.531103Z"
        },
        "trusted": true,
        "id": "TUtCKw09Qzkz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The outcome is our target variable, and Patient is the variable created by us. They carry the same info, and I don't want to encode the target variable since the aim of the final model will be to predict that. The rest of the numerical columns can be encoded."
      ],
      "metadata": {
        "id": "sLouWOZOQzkz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cat_cols_final = [i for i in cat_cols if i not in [\"Patient\", \"Outcome\"]]\n",
        "df = pd.get_dummies(df, columns=cat_cols_final, drop_first=True)\n",
        "df.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-02T16:08:17.534276Z",
          "iopub.execute_input": "2022-08-02T16:08:17.534785Z",
          "iopub.status.idle": "2022-08-02T16:08:17.565432Z",
          "shell.execute_reply.started": "2022-08-02T16:08:17.534747Z",
          "shell.execute_reply": "2022-08-02T16:08:17.564037Z"
        },
        "trusted": true,
        "id": "OgEpWpE5Qzkz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Scaling\n",
        "\n",
        "It is a variable scaling operation. The aim is to eliminate the measurement difference between the variables and to try to ensure that the models to be used approach the variables under equal conditions. Tree-based methods are not affected by outlier, standardization. In the general trend, we may prefer to scale features.\n",
        "\n",
        "The variance structure and information structure within the variable itself are not deteriorated, but are set to a certain standard. For example, let's say a dataset has a value of 10 and is ranked 80th when ordered from smallest to largest. When this variable is standardized, the value of 10 will be something like between 1-2 or 0 to 1, but when the data set is again ordered from smallest to largest, this value will still correspond to the 80th value. Therefore, when a variable is standardized, the value of the variable will change, it will be put into a certain format, but the spread and the essence of the distribution information will not change. The values of the variable such as mean, standard deviation and variance will change,as well, but the spread, distribution and current state of the information it carries within the variable will not change.\n",
        "\n",
        "There are 3 common scaling method:\n",
        "* **Standard Scaler:** Normalization. Subtract the mean, divide by the standard deviation. It can be called z-standardization. z = (x - u) / s.\n",
        "> The Standard Scaler subtracts the mean from all observation units and divides it by the standard deviation.<br>\n",
        "CAUTION: Both standard deviation and mean are metrics that are affected by outliers. Therefore, if we subtract the median from all observation units and divide by the IQR, which is not affected by outliers, we will both consider the central tendency and change, and perform a more robust standardization process.<br>\n",
        "Robust Scaler is more preferable because it is resistant to outliers compared to Standard Scaler, but its use is not common. Usually Standard Scaler and MinMax are used.\n",
        "\n",
        "* **Robust Scaler:** Subtract median and divide by IQR.\n",
        "* **MinMaxScaler:** It is used to convert values between 2 desired ranges. These 2 values may also be values that are not in our dataset."
      ],
      "metadata": {
        "id": "R8T2OpdnQzkz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-02T16:08:17.567119Z",
          "iopub.execute_input": "2022-08-02T16:08:17.567677Z",
          "iopub.status.idle": "2022-08-02T16:08:17.590427Z",
          "shell.execute_reply.started": "2022-08-02T16:08:17.56763Z",
          "shell.execute_reply": "2022-08-02T16:08:17.588716Z"
        },
        "trusted": true,
        "id": "BD9ly1eCQzkz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_scale = df.copy()\n",
        "\n",
        "# standart scaler\n",
        "ss = StandardScaler()\n",
        "df_scale[\"Age_standard_scaler\"] = ss.fit_transform(df_scale[[\"Age\"]])\n",
        "df_scale.head()\n",
        "\n",
        "# robust scaler\n",
        "rs = RobustScaler()\n",
        "df_scale[\"Age_robuts_scaler\"] = rs.fit_transform(df_scale[[\"Age\"]])\n",
        "\n",
        "# min-max scaler\n",
        "# The range can be given with the feature_range=() argument\n",
        "mms = MinMaxScaler() # default range from 0 to 1\n",
        "df_scale[\"Age_min_max_scaler\"] = mms.fit_transform(df_scale[[\"Age\"]])\n",
        "\n",
        "df_scale.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-02T16:08:17.592177Z",
          "iopub.execute_input": "2022-08-02T16:08:17.593341Z",
          "iopub.status.idle": "2022-08-02T16:08:17.633246Z",
          "shell.execute_reply.started": "2022-08-02T16:08:17.59329Z",
          "shell.execute_reply": "2022-08-02T16:08:17.632227Z"
        },
        "trusted": true,
        "id": "mJs5_Ne_Qzkz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The difference between scales can be observed above. In this problem, \"robust scaler\" will be used here. There is no specific reason, simply, all scaling types aim for the same thing, but impact in different ways.\n",
        "\n",
        "Also, the process has to be applied to all numerical variables because the main point is the scaling is to remove the difference between variables' ranges."
      ],
      "metadata": {
        "id": "yTCsphRtQzkz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rs = RobustScaler()\n",
        "for i in num_cols:\n",
        "    df[i] = rs.fit_transform(df[[i]])\n",
        "df.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-02T16:08:17.635966Z",
          "iopub.execute_input": "2022-08-02T16:08:17.63709Z",
          "iopub.status.idle": "2022-08-02T16:08:17.701895Z",
          "shell.execute_reply.started": "2022-08-02T16:08:17.637054Z",
          "shell.execute_reply": "2022-08-02T16:08:17.700312Z"
        },
        "trusted": true,
        "id": "AOKcb0y7Qzkz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this way, our data set is ready to model. The model part is not on the topic of data preprocessing and feature engineering, obviously. It's just to observe the effect of data cleaning."
      ],
      "metadata": {
        "id": "Ljbq1AG0Qzkz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"6\"></a> <br><a id=\"6\"></a> <br>\n",
        "# 6. Model\n",
        "\n",
        "It is not within the scope of this topic, we are only doing it to see the model establishment while preparing the data.\n",
        "\n",
        "Patient carries the same information with Outcome. It was created as an example of feature engineering. Also, BMIns is not a real feature, it was another example of feature engineering using mathematical expression. Therefore, these 2 variables will be dropped.\n",
        "\n",
        "KNN algorithm will be used."
      ],
      "metadata": {
        "id": "IkZbNtgPQzkz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y = df[\"Outcome\"]\n",
        "X = df.drop([\"Outcome\", \"Patient\", \"BMIns\", \"BMInsGroup_Medium\", \"BMInsGroup_High\"], axis=1)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=17)\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "rf_model = RandomForestClassifier(random_state=46).fit(X_train, y_train)\n",
        "y_pred = rf_model.predict(X_test)\n",
        "accuracy_score(y_pred, y_test)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-02T16:08:17.70382Z",
          "iopub.execute_input": "2022-08-02T16:08:17.704342Z",
          "iopub.status.idle": "2022-08-02T16:08:18.014741Z",
          "shell.execute_reply.started": "2022-08-02T16:08:17.704291Z",
          "shell.execute_reply": "2022-08-02T16:08:18.013328Z"
        },
        "trusted": true,
        "id": "UFA5vSRTQzkz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What would be the score without any adjustments?\n"
      ],
      "metadata": {
        "id": "WeirIOwmQzkz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_no = pd.read_csv(\"../input/diabetes/diabetes.csv\")\n",
        "df_no.dropna(inplace=True) # there was no missing value\n",
        "# df_no = pd.get_dummies(df_no, columns=[], drop_first=True) # no cat. col. to encode\n",
        "\n",
        "y = df_no[\"Outcome\"]\n",
        "X = df_no.drop([\"Outcome\"], axis=1)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=17)\n",
        "rf_model = RandomForestClassifier(random_state=46).fit(X_train, y_train)\n",
        "y_pred = rf_model.predict(X_test)\n",
        "accuracy_score(y_pred, y_test)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-02T16:08:18.016483Z",
          "iopub.execute_input": "2022-08-02T16:08:18.017785Z",
          "iopub.status.idle": "2022-08-02T16:08:18.317711Z",
          "shell.execute_reply.started": "2022-08-02T16:08:18.017742Z",
          "shell.execute_reply": "2022-08-02T16:08:18.316127Z"
        },
        "trusted": true,
        "id": "VCRG_Ln2Qzkz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FINAL\n",
        "\n",
        "In this project, the steps of preprocessing and data preparation has been explained. The effect of how a dataset cleaning affect the model performance was observed at the end of the project. This is the end of the tutorial, I hope it helped. Thanks for reading!"
      ],
      "metadata": {
        "id": "YGnPTLpAQzkz"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7ZuSyzqNQzkz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}