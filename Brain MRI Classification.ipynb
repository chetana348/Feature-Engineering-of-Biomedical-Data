{
  "cells": [
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "import kagglehub\n",
        "abhranta_brain_tumor_detection_mri_path = kagglehub.dataset_download('abhranta/brain-tumor-detection-mri')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "skLiqow4ELBt"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "SNVWSf97ELBu"
      },
      "cell_type": "markdown",
      "source": [
        "# Detecting Brain Tumors by extracting features using neural networks and then using Classical ML Models for prediction\n",
        "\n",
        
      ]
    },
    {
      "metadata": {
        "id": "z8hbSLrNELBv"
      },
      "cell_type": "markdown",
      "source": [
        "<h2> DataSet Description </h2>\n",
        "\n",
        "<p> The image data that was used for this problem is <a href = \"https://www.kaggle.com/abhranta/brain-tumor-detection-mri\">Brain MRI scans for tumor detection</a>. It consists of MRI scans of the human brain. It has three sub-directories:\n",
        "        <ul>\n",
        "            <li><b>no</b> :  These are the MRI scans of the brain that have no tumors. These are labelled as 0.</li>\n",
        "            <li><b>yes</b> : These are the MRI scans of the brain that have a tumor, These are labelles as 1. </li>\n",
        "            <li><b>pred</b> : These images are the unlabelled images. These are meant to be used as the test set. </li>\n",
        "    "
      ]
    },
    {
      "metadata": {
        "id": "jt1oFNP_ELBw"
      },
      "cell_type": "markdown",
      "source": [
        "<h2> Brain Tumor</h2>\n",
        "<p>>A brain tumor is a collection, or mass, of abnormal cells in your brain. Your skull, which encloses your brain, is very rigid. Any growth inside such a restricted space can cause problems. Brain tumors can be cancerous (malignant) or noncancerous (benign). When benign or malignant tumors grow, they can cause the pressure inside your skull to increase. This can cause brain damage, and it can be life-threatening.</p>\n",
        "<img src = \"https://img.medscapestatic.com/pi/features/slideshow-slide/pediatric-brain-tumors-6009019/fig1.jpg?resize=580:*\">"
      ]
    },
    {
      "metadata": {
        "id": "WzmghV3fELBw"
      },
      "cell_type": "markdown",
      "source": [
        "<h2> MRI </h2>\n",
        "<p>Magnetic resonance imaging (MRI) is a medical imaging technique used in radiology to form pictures of the anatomy and the physiological processes of the body. MRI scanners use strong magnetic fields, magnetic field gradients, and radio waves to generate images of the organs in the body. MRI does not involve X-rays or the use of ionizing radiation, which distinguishes it from CT and PET scans. MRI is a medical application of nuclear magnetic resonance (NMR) which can also be used for imaging in other NMR applications, such as NMR spectroscopy.</p>\n",
        "<img src = \"https://i.pinimg.com/originals/49/6d/95/496d952e43d6a3b9aa87ef63a73e11fe.gif\">"
      ]
    },
    {
      "metadata": {
        "id": "Km2-FgSgELBx"
      },
      "cell_type": "markdown",
      "source": [
        "<h2> In this notebook we will be extracting the features using Neural Networks and using standard ML Classifiers to classify the data based on the extracted features. </h2>\n",
        "\n",
        "<h3>Neural networks used </h3>\n",
        "<ol>\n",
        "    <li><b>VGG-16</b></li>\n",
        "    <li><b>VGG-19</b></li>\n",
        "    <li><b>ResNet50</b></li>\n",
        "    <li><b>ResNet101</b></li>\n",
        "    <li><b>MobileNetv2</b></li>\n",
        "    <li><b>MobileNet</b></li>\n",
        "    <li><b>Inceptionv3</b></li>\n",
        "    <li><b>InceptionResnetv2</b></li>\n",
        "    <li><b>DenseNet169</b></li>\n",
        "    <li><b>DenseNet121</b></li>\n",
        "    <li><b>XceptionNet</b></li>\n",
        "</ol>\n",
        "<h3>Classifiers used</h3>\n",
        "<ol>\n",
        "    <li><b>ANN</b></li>\n",
        "    <li><b>SVM => SVC</b></li>\n",
        "    <li><b>Random Forest Classifier</b></li>\n",
        "    <li><b>AdaBoost Classifier</b></li>\n",
        "    <li><b>XGBoost Classifier</b></li>\n",
        "    <li><b> KNN Classifier</b></li>\n",
        "   "
      ]
    },
    {
      "metadata": {
        "id": "yBWVCcE4ELBx"
      },
      "cell_type": "markdown",
      "source": [
        "<h3>1. Import all the necessary Libraries</h3>"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "i4GxSiyPELBx"
      },
      "cell_type": "code",
      "source": [
        "from IPython.display import clear_output\n",
        "!pip install imutils"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "HwkibFZPELBy"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import scipy\n",
        "import  os\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import *\n",
        "from tensorflow.keras.optimizers import *\n",
        "from tensorflow.keras.losses import *\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.models import *\n",
        "from tensorflow.keras.callbacks import *\n",
        "from tensorflow.keras.preprocessing.image import *\n",
        "from tensorflow.keras.utils import *\n",
        "import shutil\n",
        "import itertools\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "# import pydot\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import *\n",
        "from sklearn.model_selection import *\n",
        "import tensorflow.keras.backend as K\n",
        "\n",
        "import plotly.graph_objs as go\n",
        "from plotly.offline import init_notebook_mode, iplot\n",
        "from plotly import tools\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.applications.vgg16 import VGG16, preprocess_input\n",
        "from tqdm import tqdm, tqdm_notebook\n",
        "from colorama import Fore\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from glob import glob\n",
        "from skimage.io import *\n",
        "%config Completer.use_jedi = False\n",
        "import time\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "import lightgbm as lgb\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier,RandomForestClassifier\n",
        "\n",
        "print(\"All modules have been imported\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "85SBU6W0ELBy"
      },
      "cell_type": "markdown",
      "source": [
        "Right now all images are in one folder with `yes` and `no` subfolders. I will split the data into `train`, `val` and `test` folders which makes its easier to work for me. The new folder heirarchy will look as follows:"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "HLUpQ49rELBy"
      },
      "cell_type": "code",
      "source": [
        "!apt-get install tree\n",
        "clear_output()\n",
        "# create new folders\n",
        "!mkdir TRAIN TEST VAL TRAIN/YES TRAIN/NO TEST/YES TEST/NO VAL/YES VAL/NO\n",
        "!tree -d"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "GKgSYoSAELBy"
      },
      "cell_type": "code",
      "source": [
        "IMG_PATH = \"../input/brain-tumor-detection-mri/Brain_Tumor_Detection\"\n",
        "\n",
        "# split the data by train/val/test\n",
        "ignored = {\"pred\"}\n",
        "# split the data by train/val/test\n",
        "for CLASS in os.listdir(IMG_PATH):\n",
        "    if CLASS not in ignored:\n",
        "        if not CLASS.startswith('.'):\n",
        "            IMG_NUM = len(os.listdir(IMG_PATH +\"/\"+ CLASS))\n",
        "            for (n, FILE_NAME) in enumerate(os.listdir(IMG_PATH +\"/\"+ CLASS)):\n",
        "                img = IMG_PATH+ '/' +  CLASS + '/' + FILE_NAME\n",
        "                if n < 300:\n",
        "                    shutil.copy(img, 'TEST/' + CLASS.upper() + '/' + FILE_NAME)\n",
        "                elif n < 0.8*IMG_NUM:\n",
        "                    shutil.copy(img, 'TRAIN/'+ CLASS.upper() + '/' + FILE_NAME)\n",
        "                else:\n",
        "                    shutil.copy(img, 'VAL/'+ CLASS.upper() + '/' + FILE_NAME)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6uP2rE-hELBy"
      },
      "cell_type": "markdown",
      "source": [
        "<h3>Data Importing and Preprocessing</h3>"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "z95fUMa_ELBz"
      },
      "cell_type": "code",
      "source": [
        "def load_data(dir_path, img_size=(100,100)):\n",
        "    \"\"\"\n",
        "    Load resized images as np.arrays to workspace\n",
        "    \"\"\"\n",
        "    X = []\n",
        "    y = []\n",
        "    i = 0\n",
        "    labels = dict()\n",
        "    for path in tqdm(sorted(os.listdir(dir_path))):\n",
        "        if not path.startswith('.'):\n",
        "            labels[i] = path\n",
        "            for file in os.listdir(dir_path + path):\n",
        "                if not file.startswith('.'):\n",
        "                    img = cv2.imread(dir_path + path + '/' + file)\n",
        "                    X.append(img)\n",
        "                    y.append(i)\n",
        "            i += 1\n",
        "    X = np.array(X)\n",
        "    y = np.array(y)\n",
        "    print(f'{len(X)} images loaded from {dir_path} directory.')\n",
        "    return X, y, labels\n",
        "\n",
        "\n",
        "\n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize = (6,6))\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=90)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "    cm = np.round(cm,2)\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, cm[i, j],\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "-6HQ83qNELBz"
      },
      "cell_type": "code",
      "source": [
        "TRAIN_DIR = 'TRAIN/'\n",
        "TEST_DIR = 'TEST/'\n",
        "VAL_DIR = 'VAL/'\n",
        "IMG_SIZE = (224,224)\n",
        "\n",
        "# use predefined function to load the image data into workspace\n",
        "X_train, y_train, labels = load_data(TRAIN_DIR, IMG_SIZE)\n",
        "X_test, y_test, _ = load_data(TEST_DIR, IMG_SIZE)\n",
        "X_val, y_val, _ = load_data(VAL_DIR, IMG_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "9Exnjv3fELBz"
      },
      "cell_type": "code",
      "source": [
        "y = dict()\n",
        "y[0] = []\n",
        "y[1] = []\n",
        "for set_name in (y_train, y_val, y_test):\n",
        "    y[0].append(np.sum(set_name == 0))\n",
        "    y[1].append(np.sum(set_name == 1))\n",
        "\n",
        "trace0 = go.Bar(\n",
        "    x=['Train Set', 'Validation Set', 'Test Set'],\n",
        "    y=y[0],\n",
        "    name='No',\n",
        "    marker=dict(color='#33cc33'),\n",
        "    opacity=0.7\n",
        ")\n",
        "trace1 = go.Bar(\n",
        "    x=['Train Set', 'Validation Set', 'Test Set'],\n",
        "    y=y[1],\n",
        "    name='Yes',\n",
        "    marker=dict(color='#ff3300'),\n",
        "    opacity=0.7\n",
        ")\n",
        "data = [trace0, trace1]\n",
        "layout = go.Layout(\n",
        "    title='Count of classes in each set',\n",
        "    xaxis={'title': 'Set'},\n",
        "    yaxis={'title': 'Count'}\n",
        ")\n",
        "fig = go.Figure(data, layout)\n",
        "iplot(fig)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "q7aiZ4W5ELBz"
      },
      "cell_type": "code",
      "source": [
        "def plot_samples(X, y, labels_dict, n=50):\n",
        "    \"\"\"\n",
        "    Creates a gridplot for desired number of images (n) from the specified set\n",
        "    \"\"\"\n",
        "    for index in range(len(labels_dict)):\n",
        "        imgs = X[np.argwhere(y == index)][:n]\n",
        "        j = 10\n",
        "        i = int(n/j)\n",
        "\n",
        "        plt.figure(figsize=(15,6))\n",
        "        c = 1\n",
        "        for img in imgs:\n",
        "            plt.subplot(i,j,c)\n",
        "            plt.imshow(img[0])\n",
        "\n",
        "            plt.xticks([])\n",
        "            plt.yticks([])\n",
        "            c += 1\n",
        "        plt.suptitle('Tumor: {}'.format(labels_dict[index]))\n",
        "        plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "1RCN8ctQELBz"
      },
      "cell_type": "code",
      "source": [
        "plot_samples(X_train, y_train, labels, 30)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "h0ObT8lGELBz"
      },
      "cell_type": "markdown",
      "source": [
        "As you can see, images have different width and height and diffent size of \"black corners\". Since the image size for VGG-16 imput layer is (224,224) some wide images may look weird after resizing. Histogram of ratio distributions (ratio = width/height):"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "xLNuEG8eELBz"
      },
      "cell_type": "code",
      "source": [
        "RATIO_LIST = []\n",
        "for set in (X_train, X_test, X_val):\n",
        "    for img in set:\n",
        "        RATIO_LIST.append(img.shape[1]/img.shape[0])\n",
        "\n",
        "plt.hist(RATIO_LIST)\n",
        "plt.title('Distribution of Image Ratios')\n",
        "plt.xlabel('Ratio Value')\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OfpYTd22ELB0"
      },
      "cell_type": "markdown",
      "source": [
        "<p> First I will be cropping the  images out so that they noly contain the brain and no excess data.</p>\n",
        "<p> This was done by finding extreme points in image contours<p>"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "t_3ZGdhpELB0"
      },
      "cell_type": "code",
      "source": [
        "def crop_imgs(set_name, add_pixels_value=0):\n",
        "    \"\"\"\n",
        "    Finds the extreme points on the image and crops the rectangular out of them\n",
        "    \"\"\"\n",
        "    set_new = []\n",
        "    for img in set_name:\n",
        "        gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "        gray = cv2.GaussianBlur(gray, (5, 5), 0)\n",
        "\n",
        "        # threshold the image, then perform a series of erosions +\n",
        "        # dilations to remove any small regions of noise\n",
        "        thresh = cv2.threshold(gray, 45, 255, cv2.THRESH_BINARY)[1]\n",
        "        thresh = cv2.erode(thresh, None, iterations=2)\n",
        "        thresh = cv2.dilate(thresh, None, iterations=2)\n",
        "\n",
        "        # find contours in thresholded image, then grab the largest one\n",
        "        cnts = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "        cnts = imutils.grab_contours(cnts)\n",
        "        c = max(cnts, key=cv2.contourArea)\n",
        "\n",
        "        # find the extreme points\n",
        "        extLeft = tuple(c[c[:, :, 0].argmin()][0])\n",
        "        extRight = tuple(c[c[:, :, 0].argmax()][0])\n",
        "        extTop = tuple(c[c[:, :, 1].argmin()][0])\n",
        "        extBot = tuple(c[c[:, :, 1].argmax()][0])\n",
        "\n",
        "        ADD_PIXELS = add_pixels_value\n",
        "        new_img = img[extTop[1]-ADD_PIXELS:extBot[1]+ADD_PIXELS, extLeft[0]-ADD_PIXELS:extRight[0]+ADD_PIXELS].copy()\n",
        "        set_new.append(new_img)\n",
        "\n",
        "    return np.array(set_new)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "QFrZU87bELB0"
      },
      "cell_type": "code",
      "source": [
        "import imutils\n",
        "img = cv2.imread('./VAL/NO/no852.jpg')\n",
        "img = cv2.resize(\n",
        "            img,\n",
        "            dsize=IMG_SIZE,\n",
        "            interpolation=cv2.INTER_CUBIC\n",
        "        )\n",
        "gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "gray = cv2.GaussianBlur(gray, (5, 5), 0)\n",
        "\n",
        "# threshold the image, then perform a series of erosions +\n",
        "# dilations to remove any small regions of noise\n",
        "thresh = cv2.threshold(gray, 45, 255, cv2.THRESH_BINARY)[1]\n",
        "thresh = cv2.erode(thresh, None, iterations=2)\n",
        "thresh = cv2.dilate(thresh, None, iterations=2)\n",
        "\n",
        "# find contours in thresholded image, then grab the largest one\n",
        "cnts = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "cnts = imutils.grab_contours(cnts)\n",
        "c = max(cnts, key=cv2.contourArea)\n",
        "\n",
        "# find the extreme points\n",
        "extLeft = tuple(c[c[:, :, 0].argmin()][0])\n",
        "extRight = tuple(c[c[:, :, 0].argmax()][0])\n",
        "extTop = tuple(c[c[:, :, 1].argmin()][0])\n",
        "extBot = tuple(c[c[:, :, 1].argmax()][0])\n",
        "\n",
        "# add contour on the image\n",
        "img_cnt = cv2.drawContours(img.copy(), [c], -1, (0, 255, 255), 4)\n",
        "\n",
        "# add extreme points\n",
        "img_pnt = cv2.circle(img_cnt.copy(), extLeft, 8, (0, 0, 255), -1)\n",
        "img_pnt = cv2.circle(img_pnt, extRight, 8, (0, 255, 0), -1)\n",
        "img_pnt = cv2.circle(img_pnt, extTop, 8, (255, 0, 0), -1)\n",
        "img_pnt = cv2.circle(img_pnt, extBot, 8, (255, 255, 0), -1)\n",
        "\n",
        "# crop\n",
        "ADD_PIXELS = 0\n",
        "new_img = img[extTop[1]-ADD_PIXELS:extBot[1]+ADD_PIXELS, extLeft[0]-ADD_PIXELS:extRight[0]+ADD_PIXELS].copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "V5ukIQCNELB0"
      },
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15,6))\n",
        "plt.subplot(141)\n",
        "plt.imshow(img)\n",
        "plt.xticks([])\n",
        "plt.yticks([])\n",
        "plt.title('Step 1. Get the original image')\n",
        "plt.subplot(142)\n",
        "plt.imshow(img_cnt)\n",
        "plt.xticks([])\n",
        "plt.yticks([])\n",
        "plt.title('Step 2. Find the biggest contour')\n",
        "plt.subplot(143)\n",
        "plt.imshow(img_pnt)\n",
        "plt.xticks([])\n",
        "plt.yticks([])\n",
        "plt.title('Step 3. Find the extreme points')\n",
        "plt.subplot(144)\n",
        "plt.imshow(new_img)\n",
        "plt.xticks([])\n",
        "plt.yticks([])\n",
        "plt.title('Step 4. Crop the image')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "EEE5bBh0ELB0"
      },
      "cell_type": "code",
      "source": [
        "# apply this for each set\n",
        "X_train_crop = crop_imgs(set_name=X_train)\n",
        "X_val_crop = crop_imgs(set_name=X_val)\n",
        "X_test_crop = crop_imgs(set_name=X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "LZ5TG8IcELB0"
      },
      "cell_type": "code",
      "source": [
        "plot_samples(X_train_crop, y_train, labels, 30)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "-W55GtokELB0"
      },
      "cell_type": "code",
      "source": [
        "RATIO_LIST = []\n",
        "for set in (X_train_crop, X_test_crop, X_val_crop):\n",
        "    for img in set:\n",
        "        RATIO_LIST.append(img.shape[1]/img.shape[0])\n",
        "\n",
        "plt.hist(RATIO_LIST)\n",
        "plt.title('Distribution of Image Ratios')\n",
        "plt.xlabel('Ratio Value')\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "-p5eo_ZYELB0"
      },
      "cell_type": "code",
      "source": [
        "def save_new_images(x_set, y_set, folder_name):\n",
        "    i = 0\n",
        "    for (img, imclass) in zip(x_set, y_set):\n",
        "        if imclass == 0:\n",
        "            cv2.imwrite(folder_name+'NO/'+str(i)+'.jpg', img)\n",
        "        else:\n",
        "            cv2.imwrite(folder_name+'YES/'+str(i)+'.jpg', img)\n",
        "        i += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "WnV0qylRELB0"
      },
      "cell_type": "code",
      "source": [
        "# saving new images to the folder\n",
        "!mkdir TRAIN_CROP TEST_CROP VAL_CROP TRAIN_CROP/YES TRAIN_CROP/NO TEST_CROP/YES TEST_CROP/NO VAL_CROP/YES VAL_CROP/NO\n",
        "\n",
        "save_new_images(X_train_crop, y_train, folder_name='TRAIN_CROP/')\n",
        "save_new_images(X_val_crop, y_val, folder_name='VAL_CROP/')\n",
        "save_new_images(X_test_crop, y_test, folder_name='TEST_CROP/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EO6j81NNELB0"
      },
      "cell_type": "markdown",
      "source": [
        "<h2>Resizing the Images</h2>\n",
        "\n",
        "<p> Since all the neural entwork models take input of size (224x224x3) hence all the images were interpolated using Inter Cubic interpolation and open cv </p>"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "4bJxf7d1ELB0"
      },
      "cell_type": "code",
      "source": [
        "def preprocess_imgs(set_name, img_size):\n",
        "    set_new = []\n",
        "    for img in set_name:\n",
        "        img = cv2.resize(\n",
        "            img,\n",
        "            dsize=img_size,\n",
        "            interpolation=cv2.INTER_CUBIC\n",
        "        )\n",
        "        set_new.append(preprocess_input(img))\n",
        "    return np.array(set_new)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "70iMdV2PELB0"
      },
      "cell_type": "code",
      "source": [
        "X_train_prep = preprocess_imgs(set_name=X_train_crop, img_size=IMG_SIZE)\n",
        "X_test_prep = preprocess_imgs(set_name=X_test_crop, img_size=IMG_SIZE)\n",
        "X_val_prep = preprocess_imgs(set_name=X_val_crop, img_size=IMG_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "t4frXjeOELB0"
      },
      "cell_type": "code",
      "source": [
        "plot_samples(X_train_prep, y_train, labels, 30)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "HnC9uXtNELB0"
      },
      "cell_type": "code",
      "source": [
        "RATIO_LIST = []\n",
        "for set in (X_train_prep, X_test_prep, X_val_prep):\n",
        "    for img in set:\n",
        "        RATIO_LIST.append(img.shape[1]/img.shape[0])\n",
        "\n",
        "plt.hist(RATIO_LIST)\n",
        "plt.title('Distribution of Image Ratios')\n",
        "plt.xlabel('Ratio Value')\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "D4XzB5vuELB0"
      },
      "cell_type": "markdown",
      "source": [
        "<h2>Image Augmentation</h2>\n",
        "<p> As the number of training samples is not that great, hence all the images went through a few augmentations to iirease the number of samples. </p>"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "cKMA-_DxELB0"
      },
      "cell_type": "code",
      "source": [
        "# set the paramters we want to change randomly\n",
        "demo_datagen = ImageDataGenerator(\n",
        "    rotation_range=15,\n",
        "    width_shift_range=0.05,\n",
        "    height_shift_range=0.05,\n",
        "    rescale=1./255,\n",
        "    shear_range=0.05,\n",
        "    brightness_range=[0.1, 1.5],\n",
        "    horizontal_flip=True,\n",
        "    vertical_flip=True\n",
        ")\n",
        "\n",
        "os.mkdir('preview')\n",
        "x = X_train_crop[0]\n",
        "x = x.reshape((1,) + x.shape)\n",
        "\n",
        "i = 0\n",
        "for batch in demo_datagen.flow(x, batch_size=1, save_to_dir='preview', save_prefix='aug_img', save_format='jpg'):\n",
        "    i += 1\n",
        "    if i > 20:\n",
        "        break\n",
        "\n",
        "plt.imshow(X_train_crop[0])\n",
        "plt.xticks([])\n",
        "plt.yticks([])\n",
        "plt.title('Original Image')\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(15,6))\n",
        "i = 1\n",
        "for img in os.listdir('preview/'):\n",
        "    img = cv2.cv2.imread('preview/' + img)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    plt.subplot(3,7,i)\n",
        "    plt.imshow(img)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    i += 1\n",
        "    if i > 3*7:\n",
        "        break\n",
        "plt.suptitle('Augemented Images')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "GTnekqRCELB0"
      },
      "cell_type": "code",
      "source": [
        "!rm -rf preview/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "TUt2WJxWELB1"
      },
      "cell_type": "code",
      "source": [
        "TRAIN_DIR = 'TRAIN_CROP/'\n",
        "VAL_DIR = 'VAL_CROP/'\n",
        "RANDOM_SEED = 42\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rotation_range=15,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    shear_range=0.1,\n",
        "    brightness_range=[0.5, 1.5],\n",
        "    horizontal_flip=True,\n",
        "    vertical_flip=True,\n",
        "    preprocessing_function=preprocess_input\n",
        ")\n",
        "\n",
        "test_datagen = ImageDataGenerator(\n",
        "    preprocessing_function=preprocess_input\n",
        ")\n",
        "\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    TRAIN_DIR,\n",
        "    color_mode='rgb',\n",
        "    target_size=IMG_SIZE,\n",
        "    batch_size=32,\n",
        "    class_mode='binary',\n",
        "    seed=RANDOM_SEED\n",
        ")\n",
        "\n",
        "\n",
        "validation_generator = test_datagen.flow_from_directory(\n",
        "    VAL_DIR,\n",
        "    color_mode='rgb',\n",
        "    target_size=IMG_SIZE,\n",
        "    batch_size=16,\n",
        "    class_mode='binary',\n",
        "    seed=RANDOM_SEED\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gFqCcD95ELB1"
      },
      "cell_type": "markdown",
      "source": [
        "<h2> Now we willl be creating our ML Classifiers </h2>"
      ]
    },
    {
      "metadata": {
        "id": "tPyLFy2YELB4"
      },
      "cell_type": "markdown",
      "source": [
        "<h2> Building the Classification Pipeline </h2>"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "keCLcnEuELB4"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn import metrics\n",
        "names = [\n",
        "        \"K Nearest Neighbour Classifier\",\n",
        "        'SVM',\n",
        "        \"Random Forest Classifier\",\n",
        "        \"AdaBoost Classifier\",\n",
        "        \"XGB Classifier\",\n",
        "        \"ANN Classifier\"\n",
        "         ]\n",
        "classifiers = [\n",
        "    KNeighborsClassifier(),\n",
        "    SVC(probability = True),\n",
        "    RandomForestClassifier(),\n",
        "    AdaBoostClassifier(),\n",
        "    XGBClassifier(),\n",
        "    MLPClassifier()\n",
        "        ]\n",
        "zipped_clf = zip(names,classifiers)\n",
        "def classifier_summary(pipeline, X_train, y_train, X_val, y_val,X_test,y_test):\n",
        "    sentiment_fit = pipeline.fit(X_train, y_train)\n",
        "\n",
        "    y_pred_train= sentiment_fit.predict(X_train)\n",
        "    y_pred_val = sentiment_fit.predict(X_val)\n",
        "    y_pred_test = sentiment_fit.predict(X_test)\n",
        "    y_pred_train = [1 if x>0.5 else 0 for x in y_pred_train]\n",
        "    y_pred_val = [1 if x>0.5 else 0 for x in y_pred_val]\n",
        "    y_pred_test = [1 if x>0.5 else 0 for x in y_pred_test]\n",
        "\n",
        "    train_accuracy = np.round(accuracy_score(y_train, y_pred_train),4)*100\n",
        "    train_precision = np.round(precision_score(y_train, y_pred_train, average='weighted'),4)\n",
        "    train_recall = np.round(recall_score(y_train, y_pred_train, average='weighted'),4)\n",
        "    train_F1 = np.round(f1_score(y_train, y_pred_train, average='weighted'),4)\n",
        "    train_kappa =  np.round(cohen_kappa_score(y_train, y_pred_train),4)\n",
        "\n",
        "\n",
        "    val_accuracy = np.round(accuracy_score(y_val, y_pred_val),4)*100\n",
        "    val_precision = np.round(precision_score(y_val, y_pred_val, average='weighted'),4)\n",
        "    val_recall = np.round(recall_score(y_val, y_pred_val, average='weighted'),4)\n",
        "    val_F1 = np.round(f1_score(y_val, y_pred_val, average='weighted'),4)\n",
        "    val_kappa =  np.round(cohen_kappa_score(y_val, y_pred_val),4)\n",
        "\n",
        "\n",
        "    test_accuracy = np.round(accuracy_score(y_test, y_pred_test),4)*100\n",
        "    test_precision = np.round(precision_score(y_test, y_pred_test, average='weighted'),2)\n",
        "    test_recall = np.round(recall_score(y_test, y_pred_test, average='weighted'),2)\n",
        "    test_F1 = np.round(f1_score(y_test, y_pred_test, average='weighted'),2)\n",
        "    test_kappa =  np.round(cohen_kappa_score(y_test, y_pred_test),2)\n",
        "    test_roc_auc = metrics.roc_auc_score(y_test, y_pred_test ,multi_class='ovo', average='weighted')\n",
        "\n",
        "    print()\n",
        "    print('------------------------ Train Set Metrics------------------------')\n",
        "    print()\n",
        "    print(\"Accuracy core : {}%\".format(train_accuracy))\n",
        "    confusion_mtx = confusion_matrix(y_train, y_pred_train)\n",
        "    cm = plot_confusion_matrix(confusion_mtx, classes = list(labels.items()), normalize=False)\n",
        "\n",
        "    print('------------------------ Validation Set Metrics------------------------')\n",
        "    print()\n",
        "    print(\"Accuracy score : {}%\".format(val_accuracy))\n",
        "    confusion_mtx = confusion_matrix(y_val, y_pred_val)\n",
        "    cm = plot_confusion_matrix(confusion_mtx, classes = list(labels.items()), normalize=False)\n",
        "\n",
        "    print('------------------------ Test Set Metrics------------------------')\n",
        "    print()\n",
        "    print(\"Accuracy score : {}%\".format(test_accuracy))\n",
        "    print(\"F1_score : {}\".format(test_F1))\n",
        "    print(\"Kappa Score : {} \".format(test_kappa))\n",
        "    print(\"Recall score: {}\".format(test_recall))\n",
        "    print(\"Precision score : {}\".format(test_precision))\n",
        "    print(\"ROC AUC score : {}\".format(test_roc_auc))\n",
        "    confusion_mtx = confusion_matrix(y_test, y_pred_test)\n",
        "    cm = plot_confusion_matrix(confusion_mtx, classes = list(labels.items()), normalize=False)\n",
        "\n",
        "    print('\\t\\tClassification Report:\\n', metrics.classification_report(y_test, y_pred_test))\n",
        "\n",
        "    print(\"-\"*80)\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "MJkXrsRbELB5"
      },
      "cell_type": "code",
      "source": [
        "def classifier_comparator(X_train,y_train,X_val,y_val,X_test,y_test,classifier=zipped_clf):\n",
        "    result = []\n",
        "    for n,c in classifier:\n",
        "        checker_pipeline = Pipeline([('Classifier', c)])\n",
        "        print(\"------------------------------Fitting {} on input_data-------------------------------- \".format(n))\n",
        "        #print(c)\n",
        "        classifier_summary(checker_pipeline,X_train, y_train, X_val, y_val,X_test,y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7JuQGD3dELB5"
      },
      "cell_type": "markdown",
      "source": [
        "<h2> Training </h2>\n",
        "<p> From here on, we will be using various neural networks woith pretrained Image Net weights and extracting a latent representation of our data. This representation will then be used by classical ML models such as K Nearest Neighbours, Support Vector MAchines, etc. to perform the classification task. Various metrics will then be used to evaluate the performance of the models."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "8L1HyxiQELB5"
      },
      "cell_type": "markdown",
      "source": [
        "# ResNet-50"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "oAmk8pTnELB5"
      },
      "cell_type": "code",
      "source": [
        "base_model= ResNet50(input_shape=(224,224,3), weights='imagenet', include_top=False)\n",
        "x = base_model.output\n",
        "x = Dropout(0.5)(x)\n",
        "x = Flatten()(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dense(64,kernel_initializer='he_uniform')(x)\n",
        "x = BatchNormalization()(x)\n",
        "predictions = Activation('relu')(x)\n",
        "#x = Dropout(0.5)(x)\n",
        "\n",
        "#predictions = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "model_feat = Model(inputs=base_model.input,outputs=predictions)\n",
        "\n",
        "train_features = model_feat.predict(X_train_prep)\n",
        "val_features=model_feat.predict(X_val_prep)\n",
        "test_features=model_feat.predict(X_test_prep)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "jiRM7BeWELB5"
      },
      "cell_type": "code",
      "source": [
        "classifier_comparator(train_features,y_train,val_features,y_val,test_features,y_test,classifier=zipped_clf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GC6pBMHnELB5"
      },
      "cell_type": "markdown",
      "source": [
        "# VGG-16"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "Z6zIA2CDELB5"
      },
      "cell_type": "code",
      "source": [
        "base_model= VGG16(input_shape=(224,224,3), weights='imagenet', include_top=False)\n",
        "x = base_model.output\n",
        "x = Dropout(0.5)(x)\n",
        "x = Flatten()(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dense(64,kernel_initializer='he_uniform')(x)\n",
        "x = BatchNormalization()(x)\n",
        "predictions = Activation('relu')(x)\n",
        "#x = Dropout(0.5)(x)\n",
        "#predictions = Dense(1 , activation='sigmoid')(x)\n",
        "\n",
        "model_feat = Model(inputs=base_model.input,outputs=predictions)\n",
        "\n",
        "train_features = model_feat.predict(X_train_prep)\n",
        "val_features=model_feat.predict(X_val_prep)\n",
        "test_features=model_feat.predict(X_test_prep)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "2CQ5liMtELB5"
      },
      "cell_type": "code",
      "source": [
        "classifier_comparator(train_features,y_train,val_features,y_val,test_features,y_test,classifier=zipped_clf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8qLfmMlyELB5"
      },
      "cell_type": "markdown",
      "source": [
        "# VGG-19"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "-vPTcW4xELB5"
      },
      "cell_type": "code",
      "source": [
        "base_model= VGG19(input_shape=(224,224,3), weights='imagenet', include_top=False)\n",
        "x = base_model.output\n",
        "x = Dropout(0.5)(x)\n",
        "x = Flatten()(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dense(64,kernel_initializer='he_uniform')(x)\n",
        "x = BatchNormalization()(x)\n",
        "predictions = Activation('relu')(x)\n",
        "#x = Dropout(0.5)(x)\n",
        "#predictions = Dense(3, activation='softmax')(x)\n",
        "\n",
        "model_feat = Model(inputs=base_model.input,outputs=predictions)\n",
        "\n",
        "train_features = model_feat.predict(X_train_prep)\n",
        "val_features=model_feat.predict(X_val_prep)\n",
        "test_features=model_feat.predict(X_test_prep)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "NaehxvVAELB5"
      },
      "cell_type": "code",
      "source": [
        "classifier_comparator(train_features,y_train,val_features,y_val,test_features,y_test,classifier=zipped_clf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "o7gk9ZryELB5"
      },
      "cell_type": "markdown",
      "source": [
        "# ResNet101"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "zkM3-v-EELB6"
      },
      "cell_type": "code",
      "source": [
        "base_model= ResNet101(input_shape=(224,224,3), weights='imagenet', include_top=False)\n",
        "x = base_model.output\n",
        "x = Flatten()(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dense(64,kernel_initializer='he_uniform')(x)\n",
        "x = BatchNormalization()(x)\n",
        "predictions = Activation('relu')(x)\n",
        "#x = Dropout(0.5)(x)\n",
        "#predictions = Dense(3, activation='softmax')(x)\n",
        "\n",
        "model_feat = Model(inputs=base_model.input,outputs=predictions)\n",
        "\n",
        "train_features = model_feat.predict(X_train_prep)\n",
        "val_features=model_feat.predict(X_val_prep)\n",
        "test_features=model_feat.predict(X_test_prep)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "gPtBflUMELB6"
      },
      "cell_type": "code",
      "source": [
        "classifier_comparator(train_features,y_train,val_features,y_val,test_features,y_test,classifier=zipped_clf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "M02vjQe0ELB6"
      },
      "cell_type": "markdown",
      "source": [
        "# MobileNet-V2"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "weh9jgyOELB6"
      },
      "cell_type": "code",
      "source": [
        "base_model= MobileNetV2(input_shape=(224,224,3), weights='imagenet', include_top=False)\n",
        "x = base_model.output\n",
        "x = Flatten()(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dense(64,kernel_initializer='he_uniform')(x)\n",
        "x = BatchNormalization()(x)\n",
        "predictions = Activation('relu')(x)\n",
        "#x = Dropout(0.5)(x)\n",
        "#predictions = Dense(3, activation='softmax')(x)\n",
        "\n",
        "model_feat = Model(inputs=base_model.input,outputs=predictions)\n",
        "\n",
        "train_features = model_feat.predict(X_train_prep)\n",
        "val_features=model_feat.predict(X_val_prep)\n",
        "test_features=model_feat.predict(X_test_prep)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "Rc3c6VNHELB6"
      },
      "cell_type": "code",
      "source": [
        "classifier_comparator(train_features,y_train,val_features,y_val,test_features,y_test,classifier=zipped_clf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XOG_e1UgELB6"
      },
      "cell_type": "markdown",
      "source": [
        "# MobileNet"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "TNSFWX2tELB6"
      },
      "cell_type": "code",
      "source": [
        "base_model= MobileNet(input_shape=(224,224,3), weights='imagenet', include_top=False)\n",
        "x = base_model.output\n",
        "x = Flatten()(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dense(64,kernel_initializer='he_uniform')(x)\n",
        "x = BatchNormalization()(x)\n",
        "predictions = Activation('relu')(x)\n",
        "#x = Dropout(0.5)(x)\n",
        "#predictions = Dense(3, activation='softmax')(x)\n",
        "\n",
        "model_feat = Model(inputs=base_model.input,outputs=predictions)\n",
        "\n",
        "train_features = model_feat.predict(X_train_prep)\n",
        "val_features=model_feat.predict(X_val_prep)\n",
        "test_features=model_feat.predict(X_test_prep)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "N12irdYOELB6"
      },
      "cell_type": "code",
      "source": [
        "classifier_comparator(train_features,y_train,val_features,y_val,test_features,y_test,classifier=zipped_clf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LW5rLSFVELB6"
      },
      "cell_type": "markdown",
      "source": [
        "# Inception-V3"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "4ObeQX4EELB6"
      },
      "cell_type": "code",
      "source": [
        "base_model= InceptionV3(input_shape=(224,224,3), weights='imagenet', include_top=False)\n",
        "x = base_model.output\n",
        "x = Flatten()(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dense(64,kernel_initializer='he_uniform')(x)\n",
        "x = BatchNormalization()(x)\n",
        "predictions = Activation('relu')(x)\n",
        "#x = Dropout(0.5)(x)\n",
        "#predictions = Dense(3, activation='softmax')(x)\n",
        "\n",
        "model_feat = Model(inputs=base_model.input,outputs=predictions)\n",
        "\n",
        "train_features = model_feat.predict(X_train_prep)\n",
        "val_features=model_feat.predict(X_val_prep)\n",
        "test_features=model_feat.predict(X_test_prep)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "zhFBNW3QELB6"
      },
      "cell_type": "code",
      "source": [
        "classifier_comparator(train_features,y_train,val_features,y_val,test_features,y_test,classifier=zipped_clf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yPykxiBLELB6"
      },
      "cell_type": "markdown",
      "source": [
        "# InceptionResNet-V2"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "eW45Rx6mELB6"
      },
      "cell_type": "code",
      "source": [
        "base_model= InceptionResNetV2(input_shape=(224,224,3), weights='imagenet', include_top=False)\n",
        "x = base_model.output\n",
        "x = Flatten()(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dense(64,kernel_initializer='he_uniform')(x)\n",
        "x = BatchNormalization()(x)\n",
        "predictions = Activation('relu')(x)\n",
        "#x = Dropout(0.5)(x)\n",
        "#predictions = Dense(3, activation='softmax')(x)\n",
        "\n",
        "model_feat = Model(inputs=base_model.input,outputs=predictions)\n",
        "\n",
        "train_features = model_feat.predict(X_train_prep)\n",
        "val_features=model_feat.predict(X_val_prep)\n",
        "test_features=model_feat.predict(X_test_prep)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "dV8nFNT-ELB6"
      },
      "cell_type": "code",
      "source": [
        "classifier_comparator(train_features,y_train,val_features,y_val,test_features,y_test,classifier=zipped_clf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yjGX9ZDqELB6"
      },
      "cell_type": "markdown",
      "source": [
        "# DenseNet-169"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "GNOdGjuVELB7"
      },
      "cell_type": "code",
      "source": [
        "base_model= DenseNet169(input_shape=(224,224,3), weights='imagenet', include_top=False)\n",
        "x = base_model.output\n",
        "x = Flatten()(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dense(64,kernel_initializer='he_uniform')(x)\n",
        "x = BatchNormalization()(x)\n",
        "predictions = Activation('relu')(x)\n",
        "#x = Dropout(0.5)(x)\n",
        "#predictions = Dense(3, activation='softmax')(x)\n",
        "\n",
        "model_feat = Model(inputs=base_model.input,outputs=predictions)\n",
        "\n",
        "train_features = model_feat.predict(X_train_prep)\n",
        "val_features=model_feat.predict(X_val_prep)\n",
        "test_features=model_feat.predict(X_test_prep)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "SYqUw1nCELB7"
      },
      "cell_type": "code",
      "source": [
        "classifier_comparator(train_features,y_train,val_features,y_val,test_features,y_test,classifier=zipped_clf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UhNLLnW7ELB7"
      },
      "cell_type": "markdown",
      "source": [
        "# DenseNet-121"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "KP1-qFLnELB7"
      },
      "cell_type": "code",
      "source": [
        "base_model= DenseNet121(input_shape=(224,224,3), weights='imagenet', include_top=False)\n",
        "x = base_model.output\n",
        "x = Flatten()(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dense(64,kernel_initializer='he_uniform')(x)\n",
        "x = BatchNormalization()(x)\n",
        "predictions = Activation('relu')(x)\n",
        "#x = Dropout(0.5)(x)\n",
        "#predictions = Dense(3, activation='softmax')(x)\n",
        "\n",
        "model_feat = Model(inputs=base_model.input,outputs=predictions)\n",
        "\n",
        "train_features = model_feat.predict(X_train_prep)\n",
        "val_features=model_feat.predict(X_val_prep)\n",
        "test_features=model_feat.predict(X_test_prep)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "om2QExc_ELB7"
      },
      "cell_type": "code",
      "source": [
        "classifier_comparator(train_features,y_train,val_features,y_val,test_features,y_test,classifier=zipped_clf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YJQbl4MqELB7"
      },
      "cell_type": "markdown",
      "source": [
        "# XceptionNet"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "1jNp0VNEELB7"
      },
      "cell_type": "code",
      "source": [
        "base_model= Xception(input_shape=(224,224,3), weights='imagenet', include_top=False)\n",
        "x = base_model.output\n",
        "x = Flatten()(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dense(64,kernel_initializer='he_uniform')(x)\n",
        "x = BatchNormalization()(x)\n",
        "predictions = Activation('relu')(x)\n",
        "#x = Dropout(0.5)(x)\n",
        "#predictions = Dense(3, activation='softmax')(x)\n",
        "\n",
        "model_feat = Model(inputs=base_model.input,outputs=predictions)\n",
        "\n",
        "train_features = model_feat.predict(X_train_prep)\n",
        "val_features=model_feat.predict(X_val_prep)\n",
        "test_features=model_feat.predict(X_test_prep)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "a9QL9buSELB7"
      },
      "cell_type": "code",
      "source": [
        "classifier_comparator(train_features,y_train,val_features,y_val,test_features,y_test,classifier=zipped_clf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "yEEkpsDkELB7"
      },
      "cell_type": "code",
      "source": [
        "# clean up the space\n",
        "!rm -rf TRAIN TEST VAL TRAIN_CROP TEST_CROP VAL_CROP"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "ObHLlCO5ELB7"
      },
      "cell_type": "code",
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "Feature Extraction using Neural Nets",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}